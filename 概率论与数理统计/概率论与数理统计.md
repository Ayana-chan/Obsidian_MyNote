
# 概率论概念

## 基本概念

- **总体**：所研究对象的全体
- **个体**：组成总体的每个基本单位
- **随机变量**：个体的某一项或几项指标

对总体的研究可以归结为对表征总体中个体的指标的随机变量的研究，因此可以直接说”总体$X$“是随机变量。它有分布函数$F(x)$，因此可以说“总体$F(x)$”或“正态总体”等。

- 在总体里挑出n个个体，就是**抽样**，得到的个体是**样本**。
- n称为**样本容量**。

- 在相同条件下对总体$X$进行n次重复独立观察，得到$X_1, X_2, \dots, X_n$，称为n次**简单随机抽样**（简称简单抽样）。
- 任意$X_i$都是和总体$X$有**相同分布**的<u>随机变量</u>，且**互相独立**。称随机变量$X_1, X_2, \dots, X_n$是来自总体$X$的**简单随机样本**（简称样本）。

- 抽样观察的结果是具体的数值$x_1, x_2, \dots, x_n$，称作样本观察值
- 样本所有可能取值的全体称作**样本空间**$\chi$（或$\mathcal{S}$）。例如掷骰子的样本空间是$\chi=\{1,2,3,4,5,6\}$
- 样本观察值就是样本空间里的一个点，称作**样本点**

**事件（event）** 是一组给定样本空间的随机结果。如掷骰子中的出现5（$\{5\}$）和出现奇数（$\{1, 3, 5\}$）都是事件。如果一次随机实验结果在$\mathcal{A}$中，则说事件$\mathcal{A}$已发生。

在样本空间$\chi$中事件$\mathcal{A}$的**概率**表示为$P(\mathcal{A})$。

- 概率密度函数（Probability Density Function, PDF）
- 分布函数（或累计分布函数）（Cumulative Distribution Function, CDF）

## 概率论公理

1. 非负性：$\forall \mathcal A, P(\mathcal{A}) > 0$
2. 规范性：$\forall \mathcal \chi, P(\chi) = 1$
3. 可列可加性：对于互斥事件（$\forall i \neq j, A_i \cap A_j = \emptyset$），有$P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \quad$

## 其他概念

**联合概率（joint probability）** 是多个随机变量取指定值的概率，如$P(A=a,B=b)$。恒有$P(A = a, B=b) \leq P(A=a)$。

**条件概率（conditional probability）**：在$A=a$的条件下发生$B = b$的概率:
$$P(B=b \mid A=a) = \frac{P(A=a, B=b)}{P(A=a)}$$

> [!note]
> 对于不独立的随机变量$A$和$B$，想要拆解联合概率密度$P(A, B)$，只能拆成$P(A)P(B|A)$或$P(B)P(A|B)$。

> [!note]
> $$P(A, B, C) = P(A) P(B | A) P(C | A, B)$$



**贝叶斯定理（Bayes’ theorem）**：

$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$$

**求和法则（sum rule）** 或**边际化（marginalization）**：

$$P(B) = \sum_{A} P(A, B)$$

边际化结果的概率或分布称为**边际概率（marginal probability）** 或**边际分布（marginal distribution）**。

**独立（independence）** 表示为$A \perp B$，此时$P(A \mid B) = P(A)$，即$P(A, B) = P(A)P(B)$。非独立的随机变量称作**依赖（dependence）**。

随机变量$A$和$B$在给定另一个随机变量$C$时**条件独立（conditionally independent）** 表示为$A \perp B \mid C$， 此时$P(A, B \mid C) = P(A \mid C)P(B \mid C)$。

> [!notice]
> 独立和条件独立之间**没有蕴含关系**！因为C可能与A、B都有依赖，在C下原本独立的A和B之间可能也会发生依赖。

## 期望

随机变量$X$的**期望（expectation）** 或**均值（average）** 表示为：
$$\mu = E(X) = \sum_{x} x P(X = x)$$

随机变量函数$f(x)$的期望可以这样计算：
$$E_{x \sim P}(f(x)) = \sum_x f(x) P(x)$$ ^hgahvq

运算性质： 

$$E(aX+bY+c)=aE(X)+bE(Y)+c$$

$$E(XY) = E(X)E(Y) + Cov(X, Y)$$

> [!note]
> 可知当$E(Y) = 0$时，有$E(XY) = Cov(X, Y)$。

$$X \perp Y \rightarrow E(XY) = E(X)E(Y)$$



## 方差

随机变量$X$的**方差（variance）** 表示为：

$$D(X) = \mathrm{Var}(X) = E\left((X - E(X))^2\right) =
E(X^2) - E^2(X)$$

方差的平方根被称为**标准差（standard deviation）**。

随机变量函数$f(x)$的方差为：

$$\mathrm{Var}(f(x)) = E\left(f(x) - E(f(x))\right)^2$$

运算性质：

$$D(aX+b)=a^2D(X)$$

$$D(aX+bY)=a^2D(X)+b^2D(Y)+2abCov(X,Y)$$

$$X \perp Y \rightarrow D(aX+bY)=a^2DX+b^2DY$$

> [!note]
> 当对任意$i$和$j$都有$X_i \perp X_j$时，有：
> $$D\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n D(X_i)$$


## 协方差与相关系数

**协方差（Covariance）** 用于衡量<u>两个变量的总体误差</u>。而方差是协方差在两个变量是相同时的特殊情况。

$$\begin{aligned}
Cov\left(X,Y\right)& =E\left(\left(X-E\left(X\right)\right)\left(Y-E\left(Y\right)\right)\right) \\
&=E\left(XY\right)-2E\left(Y\right)E\left(X\right)+E\left(X\right)E\left(Y\right) \\
&=E\left(XY\right)-E\left(X\right)E\left(Y\right)
\end{aligned}$$

运算性质：

1. $Cov(X, Y) = Cov(Y, X)$
2. $Cov(aX, bY) = abCov(X,Y)$
3. $Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)$
4. $Cov(X + a, Y + b) = Cov(X, Y)$

可见：
$$Cov(X, X) = D(X)$$
$$X \perp T \rightarrow Cov(X, Y) = 0$$

随机变量的加减运算后的方差可以用方差加协方差表示：
$$
\begin{align}
D(X+Y)&=D(X)+D(Y)+2Cov(X,Y)\\
D(X-Y)&=D(X)+D(Y)-2Cov(X,Y)
\end{align}
$$

可以得到：
$$Cov\biggl(\sum_{i=1}^nX_i,\sum_{j=1}^mY_j\biggr)=\sum_{i=1}^n\sum_{j=1}^m Cov(X_i,Y_j)$$
$$Var\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^n Var(X_i)+2\sum_{i,j: i<j}Cov(X_i,X_j)$$

**相关系数**定义为：
$$\rho_{XY}=\frac{Cov\left(X,Y\right)}{\sqrt{D\left(X\right)}\sqrt{D\left(Y\right)}}$$

若$\rho_{XY} = 0$，则$X$和$Y$**线性不相关**。

显然：$$\rho_{XY} = 0 \leftrightarrow Cov(X,Y) = 0$$
从而可知，<u>独立是线性不相关的**充分不必要条件**</u>。

性质：

$$|\rho_{XY}| \leq 1$$
$$|\rho_{XY}| = 1 \leftrightarrow P(Y=aX+B) = 1$$


## 矩

n阶**矩（moment）**（原点矩）被定义为一变量的n次方与其概率密度函数之积的积分：

$$v_{n}=E(X^k)
=\int x^{n}d\left(F(x)\right)
=\int x^{n}f(x)dx$$

离散情况下可以写成：
$$v_{n}=\sum x^{n}P\left(x\right)$$

1阶矩为数学期望。


n阶**中心矩（central moment）** 要让随机变量减去均值后进行积分：

$$\mu_k=\operatorname{E}((X-\operatorname{E}(X))^k)
=\int_{-\infty}^{+\infty}(x-\mu)^kd\left(F(x)\right)
=\int_{-\infty}^{+\infty}(x-\mu)^kf(x)dx$$

0阶中心矩恒为1，1阶恒为0，二阶为方差。


# 数理统计基础

## 基本概念

称决定总体分布的参数为**总体参数**（往往是有已知的分布函数类型，然后只有若干参数未知）。

总体参数所属的范围称作**参数空间**$\Theta$。对应的总体分布范围$\{P_\theta:\theta \in \Theta\}$（多种分布组成的集合）称为**总体分布族**，也称为**参数分布族**。

常见的参数分布族：
- Poisson分布族：$\{P(\lambda):\lambda > 0\}$
- 正态分布族：$\{N(\mu, \sigma^2): \mu \in \mathbb{R}, \sigma^2 > 0\}$
- 0-1（或两点）分布族：$\{B(1, p): 0 < p < 1\}$

> [!note]
> 总体$X$（分布函数为$F(x)$)的样本（独立同分布）的联合分布函数为：
> $$F(x_1, x_2, \dots, x_n) = \prod_{i=1}^n{x_i}$$

## 大数定律

**切比雪夫大数定律**：

$$\lim_{n\to\infty}P\{ |\frac{1}{n}\sum_{k=1}^{n}x_{k}-\frac{1}{n}\sum_{k=1}^{n}Ex_{k}|<\varepsilon \}=1$$

> [!tip]
> $Ex_{k}$也可以表示成$\mu_k$。

若各样本独立同分布，则有**辛钦大数定理**：
$$\lim_{n\to\infty}P\Big\{ \left| \frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu \right|<\varepsilon\Big\}=1$$

**伯努利（Bernoulli）大数定律**：设$n_{\mathcal{A}}$是n次独立试验中事件$\mathcal{A}$发生的次数，且事件$\mathcal{A}$在每次试验中发生的概率为p，则对任意正数$\epsilon$，有：

$$\lim_{n\to\infty}P(\left|\frac{n_{\mathcal{A}}}{n}-p\right|<\varepsilon)=1$$



### 推论

样本的**k阶中心矩**依概率收敛于总体的k阶中心矩，即
$$B_k\stackrel{P}{\longrightarrow}\nu_k\quad n\rightarrow+\infty,k=1,2,\cdots,m$$

## 统计量

若样本的函数$T(X_1, X_2, \dots, X_n)$不含任何未知参数，则称该函数为**统计量**。

几个常见统计量：
- 样本均值：$\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
- 样本方差（均方差）（无偏）：$S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$
- 样本k阶原点矩：$A_{k}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\left(k=1,2,\cdots,\right)$
- 样本k阶中心矩：$B_{k}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{k}\left(k=1,2,\cdots,\right)$

样本二阶原点矩可以表示为$\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}$。

证明样本方差无偏：
$$
\begin{aligned}
E(S^{2})&=E\left[\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\right]\\
&=\frac{1}{n-1}E\left[\sum_{i=1}^{n}X_{i}^{2} - 2\overline{X}\sum_{i=1}^n{X_i}+n\overline{X}^{2}\right]\\
&=\frac{1}{n-1}E\left[\sum_{i=1}^{n}X_{i}^{2}-n\overline{X}^{2}\right]\\
&=\frac{1}{n-1}\left[\sum_{i=1}^{n}E(X_{i}^{2})-nE(\overline{X}^{2})\right] \\
&=\frac1{\mathbf{n}-\mathbf{1}}\left[nE(X^2)-nE(\overline{X}^2)\right] \\
&=\frac n{n-1}\Big[E(X^2)-E(\overline{X}^2)\Big] \\
&=\frac n{n-1}\{Var(X)+[E(X)]^2-Var(\overline{X})-[E(\overline{X})]^2\} \\
&=\frac n{n-1}[Var(X)-Var(\overline{X})] \\
&=\frac n{n-1}\left[Var(X)-\frac1nVar(X)\right] \\
&=Var(X)
\end{aligned}
$$
> [!note]
> $$E(\hat\sigma^2) = \sigma^2 + \mu^2$$
> 计算方法见[计算二阶矩](概率论与数理统计/概率论与数理统计.md#计算二阶矩)。

**顺序统计量**$X_{(k)}$表示样本中第k小的那个样本。显然，它是$X_1, X_2, \dots, X_n$的函数，且不含未知参数，是一个合法的统计量。
[概率论与数理统计](概率论与数理统计/概率论与数理统计.md#^l9p7tp)
特殊的，有：
$$X_{\left(1\right)}=\min\left\{X_{1},X_{2},\cdots,X_{n}\right\}$$
$$X_{(n)}=\max\{X_{1},X_{2},\cdots,X_{n}\}$$

若总体$X$的分布函数为$F(x)$，则：

$$F_{X_{\left(1\right)}}\left(t\right)=1-\left[1-F\left(t\right)\right]^{n}$$
$$F_{X_{\left(n\right)}}\left(t\right)=F^{n}\left(t\right)$$
**样本极差**：

$$R=X_{\left(n\right)}-X_{\left(1\right)}$$

**样本中位数**：

$$
m_{0.5}=\begin{cases}X_{\left(\frac{n+1}{2}\right)},&n\text{是奇数}\\
\frac{1}{2}\left(X_{\left(\frac{n}{2}\right)}+X_{\left(\frac{n}{2}+1\right)}\right),&n\text{是偶数}\end{cases}
$$

## 充分统计量

设样本总体分布族为$\{P_{\theta}:\theta\in\Theta\}$，若给定$T(X_{1},X_{2},\cdots,X_{n})=t$时，样本$X_1, X_2, \dots, X_n$的条件分布函数$F_{\theta}\left(x_{1},x_{2},\cdots,x_{n}\mid t\right)$与参数$\theta$无关，则称统计量$T(X_1,X_2,\cdotp\cdotp\cdotp,X_n)$为参数$\theta$的**充分统计量**。定义中，“条件分布函数”可以被替换为”条件密度函数“或离散的”条件分布列“，因为它们互相一一对应，不丢失信息。

> [!note]
> 简单来说，$T(X) = T$，$P(X|T)$与$\theta$无关，则$T$为充分统计量。

> [!tip]
> 计算条件分布函数时，需要注意[带统计量的联合分布列与条件分布列](概率论与数理统计/概率论与数理统计.md#带统计量的联合分布列与条件分布列)的计算流程。

**因子分解定理**：统计量$T(x)$是**充分统计量**，*当且仅当*：<u>联合分布列或密度函数</u>$p\left(x;\theta\right)$对所有$x \in \chi$都可以被分解成：

$$p(x;\theta)=g(T(x),\theta)h(x)$$

$g$是样本空间和参数分布族的实值函数，$h$是与参数无关的样本空间上的实值函数。

> [!notice]
> $h(x)$可以是常数。

> [!notice]
> $T(x)$函数可以是元组，如$T(x) = \left(\sum_{i=1}^{n}x_{i},\sum_{i=1}^{n}x_{i}^{2}\right)$。

> [!tip]
> 因子分解求取充分统计量的一个简单流程是：如果有$x$与$\theta$难以分离，那么就想办法把这部分$x$纳入到$T(x)$中（最暴力的就是将其塞入$T(x)$的元组中）。最“坏”的情况是全部$x$都被纳入$T(x)$，使得$h(x)$与$x$无关（变成常数函数）。

> [!note]
> 一般情况下，若$T(x)$是充分统计量，$g(t)$是一一对应的实函数(可以是向量函数), 则$g(T(x))$也是充分统计量。

如果充分统计量剔除了与参数无关的信息（最严格），那么称作**最小充分统计量**。

如果$V(X) = V$，$P(V)$与$\theta$无关，则$V$为**辅助统计量**（即其分布完全与参数无关）。

## 经验分布函数

**经验频数**$v_n(x)$表示给定x，当前观察值$x_1, x_2, \dots, x_n$中不大于x的个数。这是一个随机变量。经验频数的<u>概率</u>为分布函数在x上的取值$F(x) = P(X \leq x)$。

$v_n(x)$实际上是进行了一次n重Bernoulli实验（每个样本都与x比大小，得到的是${X \leq x}$事件的出现次数），因此$v_n(x)$服从二项分布$B(n,F(x))$，即：

$$
P\left\{v_{n}\left(x\right)=k\right\}
=C_{n}^{k}\left[F\left(x\right)\right]^{k}\left[1-F\left(x\right)\right]^{n-k},\quad k=0,1,2,\cdots,n
$$

将$v_n(x)$代入[Bernoulli大数定律](概率论与数理统计/概率论与数理统计.md#大数定律)，可知：

$$\lim_{n\to+\infty}P\left\{\left|\frac{v_{n}\left(x\right)}{n}-F\left(x\right)\right|\geqslant\varepsilon\right\}=0$$

因此可知：

$$\frac{v_n(x)}{n} \stackrel{P}{\longrightarrow} F(x)$$

> [!tip]
> 虽然$v_n(x)$确实对每个样本都进行了独立的检测，但只是简单的比大小，而样本本身就可以从小到大排列。因此，总是有一部分较小的样本全部检测失败，而剩下的全部成功。

由此，可以定义**经验分布函数**：

$$
F_{n}(x)=\frac{v_{n}(x)}{n}=\begin{cases}0,&x<x_{(1)}\\
\frac{k}{n},&x_{(k)}\leqslant x<x_{(k+1)},&k=1,2,\cdots,n-1\\
1,&x\geqslant x_{(n)}\end{cases}
$$

> [!note]
> 相对地，$F(x)$就被称作**理论分布函数**。

经验分布函数是<u>顺序统计量的函数</u>。

经验分布函数单调不减，右连续，值域$I \in [0,1]$，是关于$x$的阶梯函数。

显然，其数学期望：

$$E(F_{n}(x))=E\left(\frac{v_{n}(x)}{n}\right)=F(x)$$

**Glivenko定理**：当$n \rightarrow \infty$时，经验分布函数以概率1关于$x$**一致收敛**于理论分布函数。即：

$$P\left\{\lim_{n\to\infty}\sup_{-\infty<x<\infty}\left|F_{n}\left(x\right)-F\left(x\right)\right|=0\right\}=1$$

可以利用经验分布函数，让样本矩的观察值使用积分来表示（阶梯函数的积分依然是离散的）：

$$
\begin{gathered}
\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\int_{-\infty}^{+\infty}xdF_{n}\left(x\right) \\
S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\frac{n}{n-1}\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=\frac{n}{n-1}\int_{-\infty}^{+\infty}\left(x-\overline{x}\right)^{2}dF_{n}\left(x\right) \\
A_{k}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k}=\int_{-\infty}^{+\infty}x^{k}dF_{n}\left(x\right),\quad k=2,3,\cdots \\
B_{k}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{k}=\int_{-\infty}^{+\infty}\left(x-\overline{x}\right)^{k}dF_{n}\left(x\right),\quad k=2,3,\cdots 
\end{gathered}
$$

## 抽样分布

称统计量的分布为**抽样分布**。

如果有n个样本，能确定统计量$T(x_1, x_2, \dots, x_n)$的分布函数，那么这个分布为**精确分布**。如果难以确定精确分布，也可以求$n \to \infty$时的**极限分布**。

## 条件期望

$F(x|Y)=P(X\leq x|Y)$为$X$关于事件$Y$的条件分布函数，则$X$关于$B$的**条件期望**为：
$$
\begin{align}
E(X|Y) &=\int_{\mathbb{R}}xdF_{X|Y}\left(x|Y\right) \\
&= \sum_{i=1}^\infty x_iP(X=x_i|Y) \quad 
或 \quad \int_{-\infty}^\infty xp(x|Y)dx
\end{align}
$$

1. 如果$X$和$Y$独立，则$E(X|Y)=E(X)$
2. $E(h(Y)|Y)=h(Y)$
3.  $E( q( X, Y) | Y = y) = E( q( X, y) | Y = y)$
4. 双期望定理：$E(E(X|Y))=E(X)$

$$E(h(X)|Y) = \sum_{i=1}^\infty h(x_i) P(X=x_i|Y) \quad$$

> [!note]
> 如果$E(h(X)|Y)$的$h$是示性函数$I_{s(x)}(x)$，其值只能是0或1，那么有：
> $$E(h(X)|Y) = P(s(X)|Y)$$
> 即示性函数满足的概率就等于期望。值域可穷举的$h$都能这样**将条件期望转化为条件概率**，然后通过条件概率的计算性质进行化简求值。

# 参数估计

**参数估计**，就是对来自总体的样本$x_1,x_2,\cdots,x_n$构造<u>合适的统计量</u>$T(x_1,x_2,\cdots,x_n)$作为<u>参数</u>$q(\theta)$的估计，记为
$$\widehat{q}(x_1,x_2,\cdots,x_n)=T(x_1,x_2,\cdots,x_n)$$
或简称$\widehat{q}(x)=T(x)$，其中$x = x_1,x_2,\cdots,x_n$。

> [!tip]
> 因为是用统计量作为参数的估计，因此参数的估计是样本观察值的函数。

> [!info]
> 若$\theta$频率估计、矩估计、极大似然估计分别为$\widehat{\boldsymbol{\theta}}_1,\widehat{\boldsymbol{\theta}}_2,\widehat{\boldsymbol{\theta}}_3$, 则$f(\boldsymbol{\theta})$的频率估计、矩估计、极大似然估计分别为$f(\widehat{\boldsymbol{\theta}}_1)$，$f(\widehat{\boldsymbol{\theta}}_2)$，$f(\widehat{\boldsymbol{\theta}}_3)$。
> 
> 这意味着，如果找到能代表所有参数信息的函数的话，就可以只对该函数进行估计，从而大大降低估计的复杂度。

## 频率替换估计

**频率替换估计（Frequency Substitution Estimate）**：使用出现的频率来直接作为概率的估计：
$$\widehat{p}_{i}=\frac{n_{i}}{n}$$

## 矩估计

若参数$\theta=(\theta_1,\theta_2,\cdotp\cdotp\cdotp,\theta_s)$（即有s个要估计的数），并且$r$阶原点矩$E_\theta(X^r)$存在（$r>s$），那么就可以使用各阶原点矩列出$s$个等式（不妨就是$1\sim s$阶原点矩），从而解出$s$个参数的表达式（即使用原点矩表示参数）。即：
$$\begin{cases}\boldsymbol{\theta_1}=h_1(\boldsymbol{\mu_1},\boldsymbol{\mu_2},\cdots,\boldsymbol{\mu_s})\\
\boldsymbol{\theta_2}=h_2(\boldsymbol{\mu_1},\boldsymbol{\mu_2},\cdots,\boldsymbol{\mu_s})\\
......\\
\boldsymbol{\theta_s}=h_s(\boldsymbol{\mu_1},\boldsymbol{\mu_2},\cdots,\boldsymbol{\mu_s})\end{cases}$$
然后，使用样本原点矩$A_k=\frac1n\sum_{i=1}^nX_i^k$替换相应的$\mu_k$，就能得到**矩估计（Moment Estimate）**：
$$\begin{cases}
\widehat{\boldsymbol{\theta}}_1=h_1(A_1,A_2,\cdots,A_s)\\
\widehat{\boldsymbol{\theta}}_2=h_2(A_1,A_2,\cdots,A_s)\\
......\\
\widehat{\boldsymbol{\theta}}_s=h_s(A_1,A_2,\cdots,A_s)\end{cases}$$

> [!notice]
> 矩估计在有些情况下是不能使用的：
> - 如果存在的矩不够多，甚至都不存在，就用不了。例如Cauchy分布不存在任何矩。
> - 如果对同样的参数$q(\theta)$，存在多个矩估计，也无法解出来。例如Poisson分布的均值和方差都是$\lambda$。

## 极大似然估计（MLE）

极大似然法是基于这样一个统计原理：在一次试验中，**某一事件已经发生**，则必认为**发生该事件的概率最大**（小概率事件原理）。

若总体分布族为$p(x,\theta),\theta\in\Theta$，则样本$x_1, x_2, \dots, x_n$的联合分布：
$$p_{sample}(x,\theta) = L(\theta;x_1,x_2,\cdots,x_n)=\prod_{i=1}^np(x_i,\theta)$$
称为**似然函数（Likelihood Function）**，简记为$L(\theta)$。其定义域为参数空间$\Theta$。

> [!tip]
> $L(\theta)$的$x_i$都被视为已知数，即会使用具体的样本观察值进行代入，因此其自变量只有参数$\theta$。

如果存在$\widehat{\theta}\in\Theta$,使$L(\theta)$达到最大，即
$$L(\widehat{\theta}(x);x)=\sup_{\theta\in\Theta}\{L(\theta,x)\}=\sup_{\theta\in\Theta}\left\{\prod_{i=1}^np(x_i,\theta)\right\}$$
则称此$\widehat{\theta}(x)$为参数$\theta$的**极大似然估计（Maximum Likelihood Estimate, MLE）**。它是样本观察值$x_i$的函数。

相应的，称$q\left(\widehat{\theta}(x)\right)$为$q(\theta)$的MLE。

若参数空间只有有限元，则可以通过**穷举**来完成估计。否则，可以先获取**对数似然函数**（通过对$L(\theta)$取对数来化乘为加），然后通过<u>求导</u>（对各$\theta$内分量求偏导）来完成估计，此时就可以得到**得分函数（Score Function）**：
$$S(\theta;x) = \frac{\partial\mathrm{ln}L(\theta,x)}{\partial\theta}
= \frac\partial{\partial\theta}\ln p_{sample}(x,\theta)$$

那么问题转化为求解**似然方程（Likelihood Function）**（对每个$\theta$要素分别求偏导，或者说让得分函数的所有要素为0）：
$$\frac{\partial\mathrm{ln}L(\theta,x)}{\partial\theta_\mathrm{i}}=0,i=1,2,\cdots,n$$


## 估计优良性

假设用$T(x)$作为参数$q(\theta)$的估计量，评价估计优劣的一个自然准则叫**均方误差（Меап Squared Error, MSE）**，被定义为<u>估计误差的平方的期望</u>（这个平方是写在E的里面的）（**越小越好**）：
$$MSE_\theta(T)=R(\theta,T)=E\left(T(x)-q(\theta)\right)^2$$

如果$R(\theta,T)<+\infty$,则MSE由$T$的均值和方差确定，即
$$R(\theta,T)=Var_\theta(T(x))+b^2(\theta,T)$$
其中$b(\theta,T)$为用$\boldsymbol T(\boldsymbol{x})$估计$q(\boldsymbol{\theta})$产生的**偏差（bias）**，即<u>估计的均值</u>与目标值之间的偏差：
$$b(\theta,T)=E_\theta(T(x)-q(\theta)) = E_\theta[T(x)] - q(\theta)$$

> [!note]
> 因此估计的好坏由偏差量和准确度一起衡量。

设$S(x)$和$T(x)$是参数$q(\theta)$的两个估计，如果对所有$\boldsymbol{\theta}\in\boldsymbol{\Theta}$不等式
$$R(\theta,T)\leq R(\theta,S)$$
成立，且对某些$\theta$<u>严格</u>不等式成立，则称$T$比$S$**好**，也说$S$是**非容许的（inadmissible）**。

> [!note]
> **最优估计是不存在的**，因为这要求它能在所有参数值下都最优，然而刚好取在那个参数值上的平凡估计（Trivial Estimate）就能让误差变成0，而一个估计不可能误差处处恒为0（值域不是一个点，就意味着方差必然大于0，使得MSE大于0）。

如果对所有的$\boldsymbol{\theta}\in\Theta$有
$$E(T(X))=q(\theta)$$
成立，即$b(\theta,T)=0$，则称$T(X)$是参数$q(\theta)$的**无偏估计量(Unbiased Estimate)**。

无偏估计可能不存在。如果参数$q(\theta)$的<u>无偏估计存在</u>，则称$q(\theta)$是**可估的**。一般默认就是可估。

> [!note]
> 可估参数的无偏估计是不唯一的。

> [!tip]
> 无偏估计不一定是最好的估计，即它可能是非容许的。实践中，可能要“铤而走险”考虑有偏估计来优化估计。

**在函数变换下，无偏性可能消失**，即对$\theta$而言，$\widehat{\boldsymbol{\theta}}$是无偏的，但$q(\widehat{\boldsymbol{\theta}})$可能是$q(\boldsymbol{\theta})$的有偏估计。

用$U_q$表示可估参数$q(\theta)$的**所有无偏估计组成的类**：
$$U_q=\{T(x){:}E_\theta(T(x))=q(\theta),Var_\theta(T(x))<\infty,\theta\in\Theta\}$$

## 一致最小方差无偏估计（UMVUE）


如果存在无偏估计$T(x)\in U_q$,使得对任一$S(x)\in U_q$,有
$$Var_\theta(T(x))\leq Var_\theta(S(x))$$
对所有的$\boldsymbol{\theta}\in\Theta$都成立，则称$T(x)$为$q(\theta)$的**一致最小方差无偏估计（Uniformly Minimum Variance Unbiased Estimate, UMVUE）**。即无偏前提下的最优估计。

$$U_{0}=\{T_{0}\left(x\right):E_{\theta}\left(T_{0}\left(x\right)\right)=0,Var_{\theta}\left(T_{0}\left(x\right)\right)<\infty,forall\theta\in\Theta\} $$
**存在性**：设参数$q(\theta)$是可估的，则$T(x)\in U_q$是$q(\theta)$的UMVUE的<u>充分必要条件</u>是：对任意的$T_0(x)\in U_0$，对所有的$\boldsymbol{\theta}\in\Theta$都有：
$$E_\theta(T_0(x)T(x))=0$$

> [!tip]
> 不严谨地说，因为期望是0的分布函数可以任意地加在其他分布函数上面，因此为了防止加上去之后让方差变小，就要让协方差$Cov(T_0(x), T(x)) = 0$，进而得到$E(T_0(x)T(x))=0$。

推论：线性组合：设$T_1(x)$和$T_2(x)$分别是可估函数$q_1(\theta)$和$q_2(\theta)$的UMVUE，则对任一常数$a$和$b$，$aT_1(x)+bT_2(x)$是$aq_1(\theta)+bq_2(\theta)$的UMVUE。

**唯一性**：设参数$q(\theta)$是可估的，且$T(x)$ 和$S(x)$都是$q(\theta)$的UMVUE，则对所有的$\theta\in\Theta$,有 $P_\theta \{ T( x) = S( x) \} = 1$，即UMVUE在概率1下是**唯一的**。

**Rao-Blackwell定理**：设统计模型为$\{P_\theta,\theta\in\Theta\}$，$S(x)$是其充分统计量，$\varphi(x)$是$q(\theta)$的无偏估计量，令
$$T(x)=E(\varphi(x)|S(x))$$
则$T(x)$也是$q(\theta)$的无偏估计量，且对所有的$\theta\in\Theta$,有
$$Var_\theta(T(x))\leq Var_\theta(\varphi(x))$$
> [!tip]
> 使用充分统计量来对无偏估计量进行一次条件期望，就能找到更好的无偏估计量，从而帮助寻找UMVUE。

令：
$$U_q^T=\left\{E\big(\varphi(x)|T(x)\big):all \varphi(x)\in U_q\right\}\subset U_q$$
那么假如UMVUE存在，就必然在$U_q^T$之中，因为$U_q^T$相比$U_q$剔除了更差的估计。

- 对任意$T(x)$的函数$h(T(x))$，由条件期望的性质可以得到$E(h(T(x)) | T(x)) = h(T(x))$，然后由$U_q^T$的定义有$h(T(x)) \in U_q^T$，因此所有的$T(x)$的函数都在$U_q^T$中。
- 由条件期望的性质可知$E\big(\varphi(x)|T(x)\big)$是$T(x)$的函数，因此$U_q^T$中只有$T(x)$的函数。
- 因此，$U_q^T$是由$T(x)$的所有函数组成的集合，或者说$U_q^T$是**充分统计量的函数类**。


### 完全统计量

对任意$g(t)$，如果对任意$\theta\in\Theta$都有$E_\theta(g(T))=0$成立时，必成立$g(t)\equiv0$，则称统计量$T(x)$是**完全的(Complete)**。

> [!tip]
> 或者说，不存在一个非全0的函数$g(t)$，使得对任意$\theta\in\Theta$都有$E_\theta(g(T))=0$成立。

> [!tip]
> 如果$T(x)$是非完全的，那么就存在某个函数$h(t)$可以从$T$中抽取出一个量$c$，使得这个量与$\theta$无关，那么无论$\theta$如何变化，$g(t)$都可以通过$h(t) - c$保持结果为0。

> [!note]
> 证明$T(X)$是完全统计量的时候，就令$E_\theta(g(T))=0$，然后使用[函数的期望的计算性质](概率论与数理统计/概率论与数理统计.md#^hgahvq)证明$g$只能为全0函数。


### 完全充分统计量

设$x=(x_1,x_2,\cdotp\cdotp\cdotp,x_n)$是来自总体$\{P_\theta,\theta\in\Theta\}$的一个样本，其**密度函数（即样本的联合密度函数）** 可表示为：
$$p(x,\theta)=c(\theta)h(x)\exp\left\{\sum_{k=1}^mw_kT_k(x)\right\}$$
其中$$w=w(\theta)=\left(w_1(\theta),\cdotp\cdotp\cdotp,w_m(\theta)\right)\in\Omega\subset R^m$$
$$\begin{pmatrix}T_1(x),\cdots,T_m(x)\end{pmatrix}=\begin{pmatrix}\sum_{i=1}^nT_1(x_i),\cdots,\sum_{i=1}^nT_m(x_i)\end{pmatrix}$$

如果$\Omega$有内点，则统计量$(T_1(x),\cdotp\cdotp\cdotp,T_m(x))$是**完全充分的**。

> [!note]
> 完全充分统计量是最小充分统计量，反之未必。

> [!tip]
> 也称**吃$c$灰$h$挖$w$土$T$**定理。

> [!tip]
> 指数部分相当于是一个和式，其中每一项都是**所有n个样本的函数的求和式**乘上$\theta$相关的系数，即每一项的形式都是：
> $$f(\theta) \sum_{i=1}^n{g(x_i)}$$
> 或者用指定符号表达，则第$k$项（$k \in [1, m]$）是：
> $$w_k(\theta) \sum_{i=1}^n{T_k(x_i)}$$

> [!note]
> 由于转化式子的最终目的仅仅是为了求$T$，是在指数内的，而且指数的形式是和式，因此在求正态分布等分布函数较为特殊的分布的完全统计量的时候，就可以**直接使用总体分布函数**来进行操作，而不是样本联合分布函数，因为二者最后算出来的$T$显然是一模一样的（指数内部也单纯只有$m$变大）。

> [!info]
> 于是，求完全充分统计量有两种方法：
> 1. 拿出一个充分统计量$T(X)$，然后令$E_\theta(g(T))=0$，证明$g$只能为全0函数，从而证明它是完全统计量。
> 2. 计算样本联合密度函数，然后凑成chwt定理，直接得到完全充分统计量。

> [!example]
> 见[Lehmann-Scheffe 定理](概率论与数理统计/概率论与数理统计.md#Lehmann-Scheffe%20定理)。

### Lehmann-Scheffe 定理

**Lehmann-Scheffe 定理**：设$S(x)$是完全充分统计量，$\varphi(x)$是$q(\theta)$无偏估计，则$T(x)=E_\theta(\varphi(x)|S(x))$是$q(\theta)$的**UMVUE**。进一步如果对所有$\theta\in\Theta,Var_\theta(T(x))<\infty$,则$T(x)$是$q(\theta)$唯一的UMVUE。

Lehmann-Scheffe 定理给出了两种**通过完全充分统计量**$T(x)$寻找UMVUE的方法：
1. 若$h(T(x))$是$q(\theta)$无偏统计量，则$h(T(x))$也是$q(\theta)$的UMVUE。即寻找完全充分统计量的函数使之成为$q(\theta)$的无偏估计。（凑形式）
2. 若能获得$q(\theta)$的一个无偏估计量$\varphi(x)$,则$E(\varphi(x)|T(x))$就是$q(\theta)$的UMVUE。（暴力求）

> [!note]
> 因此，要求UMVUE时，可以求完全充分统计量，然后用Lehmann-Scheffe获取UMVUE。还有一种方法是[用信息不等式判断有效估计](概率论与数理统计/概率论与数理统计.md#信息不等式)。

> [!tip]
> 方法1的$h$也是向量函数，而使用chwt求出来的完全充分统计量往往是向量，则$h(T(x))$就可以是chwt求出的向量的任意部分的函数。

> [!example]
> 问：正态分布的$\mu$和$\sigma^2$<u>都未知</u>，求$\mu$和$\sigma^2$的UMVUE。
> 
> 由于正态分布分布函数的特殊性，因此使用样本联合分布函数来套定理和直接使用其分布函数是等价的。可以得到：
> $$\begin{gathered} p(x;\theta)=\frac1{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} \\
> =\frac1{\sqrt{2\pi}\sigma}e^{-\frac{\mu^2}{2\sigma^2}}\exp\left\{\frac\mu{\sigma^2}x-\frac1{2\sigma^2}x^2\right\} 
> \end{gathered}$$
> 
> 其中：
> $$\boldsymbol{w}=\left(\frac{\mu}{\sigma^{2}},-\frac{1}{2\sigma^{2}}\right)$$
> 
> 由于$\mu$和$\sigma^2$<u>都未知</u>，都是$\theta$的一部分，因此$w$会拥有面积，是有内点的。从而得到完全充分统计量：
> $$T(x)=\left(\sum_{i=1}^nx_i,\sum_{i=1}^nx_i^2\right)$$
>  
> > [!danger]
> > 当$\mu$或$\sigma^2$的<u>其中一个已知</u>的时候，$w$的值域变成了直线，**没有内点**，因此**无法使用该式子**求此时正态分布的完全充分统计量。
> 
> 给出一个完全充分统计量的函数：
> $$h(T(x)) = \sum_{i=1}^nx_i$$
> 
> 这是$\mu$的无偏估计，因此得到$\mu$的UMVUE：
> $$\hat{\mu} = \sum_{i=1}^nx_i = \overline{x}$$
> 
> 再给出一个完全充分统计量的函数：
> $$h(T(x)) = \frac{1}{n-1}\sum_{i=1}^n(x_i-\overline{x})^2$$
> 
> 这是$\sigma^2$的无偏估计，因此得到$\sigma^2$的UMVUE：
> $$\hat\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i-\overline{x})^2 = S^2$$

> [!example]
> 问：正态分布的$\sigma^2$已知，求$\mu$的UMVUE。
> 
> $$\begin{aligned}
> p(x;\theta)
> &=\frac1{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} \\
> &=\frac1{\sqrt{2\pi}\sigma}exp\left\{-\frac{\mu^2}{2\sigma^2}\right\} \times exp\left\{-\frac{x^2}{2\sigma^2}\right\} \times exp\{\frac{\mu}{\sigma^2}x\}
> \end{aligned}$$
> 
> 其中：
> $$w = \frac{\mu}{\sigma^2}$$
> 
> $w$的值域是有内点的，因此得到完全充分统计量：
> $$T(x) = \sum_{i=1}^{n}x_i$$
> 
> 它恰好是$\mu$的无偏估计，因此得到$\mu$的UMVUE：
> $$\hat{\mu}=\overline{x}$$

> [!example]
> 问：正态分布的$\mu$已知，求$\sigma^2$的UMVUE。
> 
> $$\begin{aligned}
> p(x;\theta)
> &=\frac1{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\} \\
> &=\frac1{\sqrt{2\pi}\sigma} \times \exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}
> \end{aligned}$$
> 
> 其中：
> $$w = -\frac{1}{2\sigma^2}$$
> 
> $w$的值域是有内点的，因此得到完全充分统计量：
> $$T(x) = \sum_{i=1}^{n}(x_i - \mu)^2$$
> 
> 给出一个完全充分统计量的函数：
> $$h(T(x)) = \frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}$$
> 
> 这是$\sigma^2$的无偏估计，因此得到$\sigma^2$的UMVUE：
> $$\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}$$

## 信息不等式

> [!tip]
> 信息量反映着某个统计量对于参数的确定程度的贡献。

### Cramer-Rao族

设分布族为$\{P_{\theta},\theta\in\boldsymbol{\theta}\}$，密度函数为$p(x,\theta)$，$\Theta$为直线上的一个开区间。满足下述条件的分布族$\{P_\theta,\theta\in\boldsymbol{\theta}\}$称为**Cramer-Rao正则族（C-R正则族）**：

1. 支撑$A=\{x{:}\boldsymbol{p}(x,\boldsymbol{\theta})>\boldsymbol{0}\}$与$\boldsymbol{\theta}$无关，且对任一$x\in A$,偏导数$\frac\partial{\partial\theta}lnp(x,\theta)$存在。
2. 若对所有$\theta\in\boldsymbol{\theta}$,$T(x)$是满足$E_{\boldsymbol{\theta}}|T|<\infty$任一统计量，则对$T(x)p(x,\theta)$，积分和微分可交换次序，即
$$\begin{aligned}
\frac{\partial}{\partial\theta}\int_{-\infty}^{+\infty}...\int_{-\infty}^{+\infty}T(x)p(x,\theta)dx_1...dx_n\\
=\int_{-\infty}^{+\infty}...\int_{-\infty}^{+\infty}T(x)\frac{\partial}{\partial\theta}p(x,\theta)dx_1...dx_n
\end{aligned}$$

### Fisher信息量

只要当Cramer-Rao族的条件中的<u>1成立</u>时，我们就可以定义所谓的**Fisher信息量（Fisher Information Number）**：
$$I(\theta)=E\Big[\frac\partial{\partial\theta}\ln p(x,\theta)\Big]^2(0\leq I(\theta)\leq\infty)$$

> [!tip]
> 里面的$\frac\partial{\partial\theta}\ln p(x,\theta)$的形式就是[极大似然估计](概率论与数理统计/概率论与数理统计.md#极大似然估计)的时候所用的得分函数（当$x$是所有样本时就完全等于得分函数了）。因此Fisher信息量可以看做是得分函数的二阶矩。

如果下面这个特殊的积分微分交换成立的话：
$$\frac{d^2}{d\theta^2}\int_{-\infty}^{+\infty}p(x;\theta)dx=\int_{-\infty}^{+\infty}\frac{\partial^2p(x;\theta)}{\partial\theta^2}dx$$
就可以证明（即可以使用二阶导求信息量）：
$$I(\theta)=-E_\theta\left[\frac{\partial^2}{\partial\theta^2}\ln p(x;\theta)\right]$$

> [!example]
> 设总体分布是Poisson分布族，即
> $$p(x,\theta)=\frac{\theta^x}{x!}e^{-\theta},x=0,1,\dots$$
> 
> 则：
> $$\frac\partial{\partial\theta}\ln p(x,\theta)=\frac x\theta-1$$
> 
> 从而得到：
> $$I(\theta)=E\left(\frac x\theta-1\right)^2=Var\left(\frac x\theta\right)=\frac1\theta$$


如果$X_1,X_2,...,X_n$是来自总体的样本，可以证明这些样本的信息量为：
$$I(\theta)=I_n(\theta)=nI_1(\theta)$$
其中$$I_1(\theta)=E(\frac\partial{\partial\theta}\ln p(X_1,\theta))^2$$

> [!tip]
> 这说明样本增加时，信息量线性增加。

> [!notice]
> $I(\theta)$常常就是在指代$I_n(\theta)$。通常将$I_1(\theta)$看成一次观察所能获得的关于参数$\theta$的信息，即一个观测值$x_1$所含$\theta$的信息，那么$I(\theta)$就表示样本$X_1,X_2,\dots,X_n$所含的信息。

> [!info]
> 有时候用$I(\theta)$表示总体本身的信息量，它等于这里的$I_1(\theta)$。但是这样表示的话$I(\theta)$就不直接和统计量发生关系，有点不合法。

统计量$T(x_1,x_2,\dots,x_n)$的信息量表示为:
$$I_T(\theta)=E_\theta\left[\frac\partial{\partial\theta}\ln p_T(t;\theta)\right]^2$$

>[!tip]
>直观上，应有$I_T(\theta) < I(\theta)$，因为统计量往往会做信息压缩（实际上，这个想法在某些条件下确实是对的。

### 信息不等式

**信息不等式（Information Inequality）**：设$T(x)$是对$\theta\in\Theta$所有满足$Var_\theta(T(X))<+\infty$的统计量，记$\varphi(\theta)=E_\theta(T(X))$。如果分布族是Cramer-Rao正则族，且$0<I(\theta)<+\infty$，则对所有的$\theta\in\Theta$，$\varphi(\theta)$是可微的，且
$$Var_\theta(T(X)) \geq 
\frac{(\varphi^\prime(\theta))^2}{I(\theta)}
= \frac{(\varphi^\prime(\theta))^2}{nI_1(\theta)}$$

当$T(X)$是$q(\theta)$的无偏估计（$T(X) \in U_q$）时，有：
$$Var_{\theta}\left(T\left(X\right)\right)\geqslant\frac{\left[q^{\prime}\left(\theta\right)\right]^{2}}{I\left(\theta\right)}$$


特别地，当$q(\theta)=\theta$时，对任一$T(X)\in U_\theta$,有
$$Var_\theta(T(X))\geq\frac1{I(\theta)}$$

通常称$\frac1{I(\theta)}$为**Cramer-Rao下界**。

### 有效估计

设分布族$\{P_{\theta},\theta\in\Theta\}$是C-R正则族，$q(\theta)$ 是可估参数，如果存在某**无偏**估计$\widehat{q}(X)\in U_q$，其方差达到信息不等式的**下界**，即
$$Var(\widehat{q}(X))=\frac{(q^\prime(\theta))^2}{I(\theta)}$$
则称$\widehat{q}(X)$为$q(\boldsymbol\theta)$的**有效估计（Efficient Estimate）**。

**有效估计必为UMVUE，反之不一定。**

---

对参数$q(\boldsymbol{\theta})$的任一无偏估计$\widehat{q}(X)\in U_q$,令
$$e(\widehat q(X))=\frac{(q^{\prime}(\theta))^2}{I(\theta)}/Var(\widehat q(X))$$
则称$e(\widehat{q}(X))$为估计$q(\theta)$的**有效率（Efficiency）**。显然$0\leq e(\widehat{q}(X))\leq1$。因此，<u>有效估计是有效率为1的无偏估计</u>。

> [!info]
> 理论上可以证明只有在指数分布族
> $$\left\{p(x;\theta)=c(\theta)exp\left\{\sum_{k=1}^m\omega_k(\theta)T_k(\theta)\right\}h(x),\theta\in\mathbf{0}\right\}$$
> 下，可估参数的有效估计才可能存在，且也并非任一可估参数都有有效估计，也就是说即使在指数分布族下，有效估计存在的情况少之又少。

### 渐进无偏/有效估计

设$\{T_n(X)\}$是参数$q(\theta)$的估计序列，如果对所有的$\boldsymbol\theta\in\Theta$，都有$\lim\limits_{n\to\infty} E_{\boldsymbol{\theta}}(T_n(X))=q(\theta)$，则称$T_n(X)$为参数$q(\theta)$的**渐进无偏估计（Asymptotic Unbiased Estimate）**。

> [!example]
> 对正态总体$N(\mu,\sigma^2)$,我们知道$\widehat{\sigma}_n^2$是总体方差$\sigma^2$的有偏估计，但样本数$n$足够大后就趋向无偏，因此是渐进无偏估计。

设$q(\theta)$是可估参数，如果存在无偏估计序列$\widehat{q}_n(X)\in U_q$,使得
$$\lim_{n\to\infty}e(\widehat{q}(X))=\lim_{n\to\infty}\frac{\left(q^{\prime}(\theta)\right)^2}{I(\theta)}/Var(\widehat{q}(X))=1$$
成立，则称$\widehat{q}_n(X)$为$q(\theta)$的**渐进有效估计（Asymptotic Efficient Estimate）**。

## 相合估计

设$\widehat{q}_n(X)=\widehat{q}_n(X_1,...,X_n)$是参数$q(\theta)$的任一估计序列，如果$\{\widehat{q}_n\}$依概率收敛于参数真值$q(\theta)$，即对任意的$\varepsilon>0$,有
$$\lim_{n\to\infty}P\{|\widehat{q}_n(X)-q(\theta)| >\varepsilon\}=0 \quad\text{即}\quad \widehat{q}_n(X) \overset{\mathrm{p}}\to q(\theta)$$
则称$\widehat{q}_n(X)$是$\boldsymbol q(\boldsymbol\theta)$的**相合估计（Consistent Estimate）**。相合性只是反映了$n\to\infty$时估计量的性质，即**大样本性质**，当样本容量有限时是无意义的。一般情形下证明估计的相合性可使用定义或大数定律。

> [!info]
> 相合估计也叫弱相合估计，是依概率收敛的；而强相合估计是几乎必然收敛。

相合分布的**优劣**往往可通过比较其<u>渐进分布的渐进方差的大小</u>来进行，最常用的渐进分布为正态分布。

设$\widehat{q}_n(X)=\widehat{q}_n(X_1,...,X_n)$是$q(\theta)$估计序列，如果存在数列$\mu_n(\theta)$和$\sigma_n^2(\theta)$，对任意实数$x$,都有
$$\lim_{n\to\infty}P\left\{\frac{\widehat{q}_n(X)-\mu_n(\theta)}{\sigma_n(\theta)}\leq x\right\}=\frac1{\sqrt{2\pi}}\int_{-\infty}^x\exp\left\{-\frac{u^2}2\right\}du$$
则称$\widehat{q}_n$具有渐近正态性，简记为
$$\frac{\widehat{q}_{n}(X)-\mu_{n}(\theta)}{\sigma_{n}(\theta)}\overset{L}{\operatorname*{\to}}N(0,1)$$

一般情形下，可取
$$\mu_n(\theta)=E\big(\widehat{q}_n(X)\big),\sigma_n^2(\theta)=Var(\widehat{q}_n(X))$$





# 分布

## 两点（0-1）（Bernoulli）分布

若$X \sim B(1, p)$，则$X$符合两点分布。$X$有$p$的概率为$0$，以$1-p$的概率为$1$。

其分布列可以表示成：
$$P\{X=k\}=p^k(1-p)^{1-k},\quad k=0,1$$

有$E(X) = p$，$D(X) = p(1-p)$。

## 二项分布（n重Bernoulli实验）

若$X \sim B(n, p)$，则$X$符合二项分布，相当于进行$n$次二项分布并求$1$的数量。

其分布列为：
$$P\{X=k\}=\begin{pmatrix}n\\k\end{pmatrix}p^k(1-p)^{n-k},k=0,1,\cdots,n$$

有$E(X) = np$，$D(X) = np(1-p)$。


## 泊松（Poisson）分布

假设单位时间内随机事件的平均发生次数为$\lambda$，令单位时间内事件发生次数为$X$，则$X$服从Poisson分布，即$X \sim P(\lambda)$：
$$P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda},k=0,1,\cdots$$
有：$E(X) = Var(X) = \lambda$

## 指数分布

假设单位时间内随机事件的平均发生次数为$\lambda$，令两次事件间的时间间隔为$X$，则$X$服从指数分布，即$X \sim Exp(\lambda)$：
$$f(x)=\left\{
\begin{array}{ll}\lambda\mathrm{e}^{-\lambda x},&x\geqslant0,\\
0,&x<0.
\end{array}\right.$$
$$F(x)=\left\{\begin{array}{ll}1-\mathrm{e}^{-\lambda x},&x\geqslant0,\\0,&x<0.\end{array}\right.$$

有：$E(X) = \frac 1 \lambda$，$D(X) = \frac{1}{\lambda^2}$

## 正态分布

$X\sim N\left(\mu,\sigma^{2}\right)$的概率密度函数为：
$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}$$

标准正态分布$X\sim N\left(0,1\right)$的概率密度函数为：
$$f(x)=\frac{1}{\sqrt{2\pi}}e^{\left(-\frac{x^2}{2}\right)}$$

若$X\sim N\left(\mu,\sigma^{2}\right)$，则$Y=\frac{X-\mu}{\sigma}\sim N\left(0,1\right)$。

如果$X\sim N(\mu,\sigma^2)$ ，则$aX+b\sim N(a\mu+b,(a\sigma)^2)$（即线性转换后还是正态分布，因此分别算均值和方差即可）。

如果$X\sim N(\mu_1,\sigma_1^2)$ ，$Y\sim N(\mu_2,\sigma_2^2)$ ，且$X \perp Y$，则（即正态分布加减运算后还是正态分布，也是分别算均值和方差即可）：

$$U=X+Y\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$$
$$V=X-Y\sim N(\mu_1-\mu_2,\sigma_1^2+\sigma_2^2)$$ 

推论：对于$X\sim N\left(\mu,\sigma^{2}\right)$的简单样本$X_1, X_2, \dots, X_n$，令：
$$Y=a_{1}X_{1}+a_{2}X_{2}+\cdots+a_{n}X_{n}$$

则有：
$$Y\sim N\left(\mu\sum_{k=1}^{n}a_{k},\sigma^{2}\sum_{k=1}^{n}a_{k}^{2}\right)$$

推论：正态分布的样本均值$\overline{X} \sim N(\mu, \frac{\sigma^2}{n})$。

正态分布$X\sim N\left(\mu,\sigma^{2}\right)$的样本均值和样本方差独立。

$$\frac{\left(n-1\right)S^{2}}{\sigma^{2}}\sim\chi^{2}\left(n-1\right)$$
$$\frac{\overline{X}-\mu}{S/\sqrt{n}}\sim t\left(n-1\right)$$
$$\frac{(\overline{X}-\mu)^{2}}{S^{2}/n}\sim F\left(1,n-1\right)$$

$$\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}\sim\chi^{2}\left(n\right)$$

如果$X\sim N(\mu_1,\sigma_1^2)$ ，$Y\sim N(\mu_2,\sigma_2^2)$ ，且$X \perp Y$，则：
$$F=\frac{S_{1}^{2}/\sigma_{1}^{2}}{S_{2}^{2}/\sigma_{2}^{2}}\sim F(n_{1}-1,n_{2}-1)$$
## 对数正态分布

如果$Y$为正态分布，则$e^Y$为对数正态分布。
$$f(x;\mu,\sigma^{2})=\begin{cases}\frac{1}{\sqrt{2\pi}\sigma x}\exp\left\{-\frac{(\ln x-\mu)^{2}}{2\sigma^{2}}\right\},&x>0\\0,&x\leqslant0\end{cases}$$
有：$\mathrm{E}(X)=e^{\mu+\sigma^2/2}$，$Var(X)=(e^{\sigma^2}-1)e^{2\mu+\sigma^2}$

$$E(X^2) = e^{2\mu+2\sigma^2}$$
## 卡方（$\chi^2$）分布

定义1.3.2 设$X_1,X_2,\cdotp\cdotp\cdotp,X_n$ 是相互独立的随机变量，且$X_i\sim N(0,1)$,则称随机变量
$$\chi^{2}=X_{1}^{2}+X_{2}^{2}+\cdots+X_{n}^{2}$$
所服从的分布是自由度为$n$ 的$\chi^2$ 分布，记为$\chi^2\sim\chi^2(n)$。

> [!notice]
> $\chi^2$是一个随机变量符号。$\chi^2(n)$是卡方分布的表示。

$\chi^2(n)$的概率密度函数为：
$$f(x)=\begin{cases}
\frac{1}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}\mathrm{e}^{-\frac{x}{2}},&x>0\\
0,&x\leqslant0
\end{cases}$$

若$X\sim\chi^{2}\left(n\right)$，则$E(X) = n$，$Var(X) = 2n$。

设$X_1,X_2,\cdotp\cdotp\cdotp,X_n$是来自正态总体$N(\mu,\sigma^2)$的简单样本，则有：
$$\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}\sim\chi^{2}\left(n\right)$$

可加性：若$X\sim\chi^{2}\left(n_{1}\right)$，$Y\sim\chi^{2}\left(n_{2}\right)$，且$X \perp Y$，则：
$$X+Y\sim\chi^{2}\left(n_{1}+n_{2}\right)$$

> [!note]
> 由可加性可知，若$X_i\sim\chi^{2}\left(n_i\right)$且各$X_i$互相独立，则：
> $$\sum_{i=1}^{k}X_{i}\sim\chi^{2}\left(\sum_{i=1}^{k}n_{i}\right)$$


## $t$分布

设$X\sim N(0,1)$，$Y\sim\chi^2(n)$,且$X$与$Y$相互独立，则称随机变量
$$T=\frac{X}{\sqrt{Y/n}}$$
所服从的分布是自由度为$n$的$t$分布，记为$T\sim t(n)$

> [!note]
> 记忆：“正态分布”除以“正态分布的平方和的根号”，最后量纲是1。

$t(n)$的概率密度函数为：
$$f(t)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{t^{2}}{n}\right)^{-\frac{n+1}{2}}$$
$t$分布的分布函数是一个偶函数，且在$n \to \infty$的时候的极限分布为标准正态分布$N(0, 1)$。

设$X\sim N(\mu,\sigma^{2})$，$\frac{Y}{\sigma^{2}}\sim\chi^{2}(n)$,且$X$与$Y$相互独立，则有：
$$T=\frac{X-\mu}{\sqrt{Y/n}}\sim t\left(n\right)$$

## $F$分布

设$X\sim\chi^2(n_1),Y\sim\chi^2(n_2)$,且$X$与$Y$相互独立，则称随机变量
$$F=\frac{X/n_{1}}{Y/n_{2}}$$
所服从的分布是自由度为($n_1,n_2$)的$F$分布，记为$F\sim F(n_1,n_2)$。

$F(n_1,n_2)$的概率密度函数为：
$$f(z)=\begin{cases}\frac{\Gamma\left(\frac{n_1+n_2}{2}\right)}{\Gamma\left(\frac{n_1}{2}\right)\Gamma\left(\frac{n_2}{2}\right)}\biggl(\frac{n_1}{n_2}\biggr)\biggl(\frac{n_1}{n_2}z\biggr)^{\frac{n_1}{2}-1} \biggl(1+\frac{n_1}{n_2}z\biggr)^{-\frac{n_1+n_2}{2}},&z>0\\0,&z\leqslant0\end{cases}$$

若$F\sim F\left(n_{1},n_{2}\right)$，则$\frac{1}{F}\sim F\left(n_{2},n_{1}\right)$。


# 特殊函数

## 示性函数

在满足要求时为1，否则为0。

$$I_{A}\left(u\right)=\begin{cases}1,&u\in A\\0,&u\notin A\end{cases}$$
例如，限定区间范围的示性函数：

$$I_{\left\langle 5\leqslant t_{1}\leqslant t_{2}\leqslant 9\right\rangle}\left(t_{1},t_{2}\right)$$


## $\Gamma$函数

$$\Gamma(\alpha)=\int_{0}^{+\infty}x^{\alpha-1}e^{-x}dx\text{(}\alpha>0\text{)}$$

它有性质：

$$\Gamma(\alpha) = \alpha\Gamma(\alpha-1)$$

重要的一个值：
$$\Gamma(\frac1 2) = \sqrt{\pi}$$


# 杂项知识点与注意点

## 不同的收敛

设$n$个样本提供了统计量$X_n$；被估的值为$X$。

### 依概率收敛(Convergence in Probability)

记作：
$$X_n\overset{\mathrm{p}}{\operatorname*{\to}}X_0$$

定义为：对于$\forall\varepsilon>0$ 有（越界是小概率事件）：
$$\lim_{n\to\infty}\:P\left\{\left|X_n-X\right|\geq\varepsilon\right\}=0$$
或（极大概率在界内）
$$\lim_{n\to\infty} P\left\{|X_n-X|\leq\varepsilon\right\}=1$$

即当$n$趋于无穷大时，统计量分布于被估值附近某个固定区间的**概率收敛于1**。

仅仅表示统计量在$n$变大的过程中跳出目标区间的概率越来越小直到几乎不可能，但并没有限制统计量在该过程中的分布情况。比如，在$n$比较小的时候基本在10附近也很可能在900附近，而$n$特别大的时候几乎都在10附近但也允许小概率出现在900附近。

因此依概率收敛时，**统计量可能不是收敛于被估值的**，因为收敛的定义要求能够找到数列上的$[a, +\infty)$项，这些项的值有上界；但<u>此时的统计量的上界没有被界定</u>。

### 几乎必然收敛（Almost Sure Convergence)

> 也叫**几乎确信收敛**。

记作：
$$X_n\to X_0\quad a.s.$$

定义为：
$$P\left\{\lim_{n\to\infty}X_n=X\right\}=1$$

即当$n$趋于无穷大时，**必然**有统计量收敛于被估值。

当$n$不断趋于无穷大时，统计量随之不断变化形成一个队列，而几乎必然收敛则要求这个队列必然收敛于被估值。这就要求统计量在$n$变大的过程中要越来越贴近被估值，即使是无穷小概率也不能出现上界飘忽的值。

### 依分布收敛 (Convergence in Distribution)

$\{X_n\}$依分布收敛于$\{X_0\}$记作：
$$X_n\overset{\mathrm{d}}{\operatorname*{\to}}X_0$$

定义为：若$\{X_n\}$和$\{X_0\}$的**分布函数**分别为$\{F_n(\cdot)\}$的$\left\{F_0(\cdot)\right\}$，若在$F_0(x)$连续处，对于所有的$x\in\Re$都有
$$\lim_{n\to\infty}F_n(x)=F_0(x)$$

要求的是当$n$不断趋于无穷大时，统计量的分布函数要收敛成目标分布函数。这不仅仅要求其统计量计算值的相关性质，还要求了**统计量计算值的分布性质**。

## 指数书写位置

$x+y$的平方的期望可以写成$E((x+y)^2)$，也可以省略外部括号写成$E(x+y)^2$。这要区别于$x+y$的期望的平方$E^2(x+y)$。

## 独立随机变量和的概率密度

使用卷积公式求取（其实就是相乘求单一情况概率，然后求和）：
$$f_{X+Y}\left(z\right)=\int_{-\infty}^{+\infty}f_{X}\left(x\right)f_{Y}\left(z-x\right)dx$$

## 带统计量的联合分布列与条件分布列

假设$X_1, ..., X_n$的**联合分布列**为：

$$P\{ X_1 = x_1, \cdots, X_n = x_n \} = 10 \sum_{i=1}^n{x_i}$$

则求附加了统计量$T = \sum_{i=1}^n{x_i} = t$时的分布列时，可以**直接代入**：

$$P\{ X_1 = x_1, \cdots, X_n = x_n, \sum_{i=1}^n{x_i} = t \} = 10t$$

因为联合分布列本来就是求一个完全具体的情况的概率，只受随机变量影响。t的加入不会引入额外的随机变量，只会影响$X_1, ..., X_n$的取值情况；而$X_1, ..., X_n$的任意取值的概率都只会被表示成$10 \sum_{i=1}^n{x_i}$，因此t可以直接代入。

若想求$T = \sum_{i=1}^n{x_i} = t$下的**条件分布列**，则简单进行除法即可：

$$P\{ X_1 = x_1, \cdots, X_n = x_n | t \} = \frac{P\{ X_1 = x_1, \cdots, X_n = x_n, \sum_{i=1}^n{x_i} = t \}}{P\{ \sum_{i=1}^n{x_i} = t \}}$$

而$P\{ \sum_{i=1}^n{x_i} = t \}$就要把$t$考虑成随机变量了，表示任意的$X$取值下其和刚好等于$t$的概率，因此要使用具体的$X$的分布列来计算$t$的分布列。

## 计算二阶矩

知道均值和方差后，可以通过方差的公式去计算二阶矩，以方便矩估计：
$$E(X^2) = D(X) + E^2(X)$$






















