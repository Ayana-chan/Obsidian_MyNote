


# 基础

A **parallel computer** is <u>a collection of</u> **processing elements** that <u>cooperate</u> to solve problems <u>quickly</u>

加速比定义：
$$speedup_P=\frac{execute\_time_{1}}{execute\_time_{P}}$$ 

# Lec2 Modern Multicore Processor

ILP: Instruction Level Parallelism

vector: 例如，把八个数据和其他地方的八个数据加在一起生成八个数据，则称数据被聚合为向量。

比如，要对一个数组的每一项都完全独立地执行一个操作，则可以让八个元素一起被拿出来，然后一起被执行一样的流程操作。

SIMD扩展（AVX intrinsics）：对advanced vector的支持。xmm寄存器是特殊的256bit（32Byte）寄存器，有完全不同的指令集。

>SIMD(Single Instruction Multiple Data)（读作same D）即单指令流多数据流，是一种采用一个控制器来控制多个处理器，同时对一组数据（又称“数据向量”）中的每一个分别执行相同的操作从而实现空间上的并行性的技术。简单来说就是一个指令能够同时处理多个数据。

>ps = pack single

![](assets/uTools_1698031596956.png)

当各个数据的操作存在分支，即操作方式不一样时，可以使用mask来实现分支，“X”表示当前不对该数据进行操作。

![](assets/uTools_1698037591707.png)

- converge: 都做同一件事，从而能最大利用。SIMD追求converge。
- diverge: 分歧。

- **explicit SIMD**: SIMD parallelization is performed at compile time. Can inspect program binary and see instructions(vstoreps,vmulps,etc.)
- **Implicit SIMD**:
	- Compiler generates a scalar binary(scalar instructions). 
	- But N instances of the program are always run together on the processor. `execute(my_function,N); //execute my_function N times`
	- In other words,the interface to the hardware itself is data-parallel.
	- Hardware(not compiler)is responsible for simultaneously executing the same instruction from multiple instances on different data on SIMD ALUs

SIMD width: 可处理的数据量（不是字节数），8~32，一般为32。

stall: 发生数据相关时，会导致运行暂停。而stall的来源的大头是内存访问。内存传输过慢，会生成较大的latency。而cache可以减少暂停的时间（latency）。**prefetch**将数据提前放到cache里面也可以减少stall。

使用**multi-threading**可以在不同thread的stall期间执行其他thread的指令；可以直接在cache-L1里面存thread execution context；线程多的时候hide stall的能力会更强。`hyper-threading`: CPU给多个线程分配不同的寄存器组，实现无需上下文切换的多线程。

Execution Unit用于根据相关性协调决定多个thread的运作方式（顺序）：
![](assets/uTools_1698041113102.png)

GPU尽力拉高吞吐量，来hide latency，达成极高的并发度；缩小cache来把更多空间用于ALU。
![](assets/uTools_1698041605328.png)


# Lec3 Parallel Programming Abstractions

## ISPC (Intel SPMD Program Compiler)

- ISPC: Intel SPMD Program Compiler，一种语言
- SPMD: single program multiple data，同代码块不同数据
- http://ispc.github.com/

**ISPC会编译出SIMD指令**。SPMD是model，SIMD是implement。由于使用SIMD来增加并发性，因此实际上是单线程的。

每次调用函数的时候，ISPC都会创建“并发”任务。

![](assets/uTools_1699235617168.png)

ISPC Keywords:
**programCount**: number of simultaneously executing instances in the gang(uniform value). 决定了并发量。
**programIndex**: id of the current instance in the gang.(a non-uniform value:"varying"). 并发的每个程序（instance）都具有不同的programIndex。$[0,programCount)$
**uniform**: A type modifier.All instances have the same value for this variable.Its use is purely an optimization.Not needed for correctness. 标记对并发的每个程序都等同的变量，用于优化。

### interleaved & blocked

**interleaved assignment**: 每个instance的任务是分散的，因此并发时，每个instance完成一个任务后，得到的结果是连续的。（是上面的ISPC代码所决定的）
![](assets/uTools_1699236729496.png)
![](assets/uTools_1699237488010.png)

**blocked assignment**: 每个instance的任务是连续的，但并发的所有instance的第一个任务是分散的。
![400](assets/uTools_1699236964851.png)
![](assets/uTools_1699237112195.png)

**interleaved比blocked更优越**，因为它每次都会得到**连续的结果**，而SIMD会把这结果放进**向量寄存器**，如果寄存器内数据是连续的话可以增加效率。blocked需要存储不连续的向量，会消耗更多CPU周期。（blocked看起来对cache更友好，但由于ISPC仅仅是单线程，因此无大用）

### foreach

普通的for循环是imperative，描述如何分配任务；而foreach是declarative，仅仅描述任务的集合。

可以直接使用foreach，将interleaved & blocked的选择交给编译器：
![](assets/uTools_1699238004114.png)

### reduction

求和时，因为结果是共享、可写的，因此标为uniform会编译出错。

解决方法：定义非uniform的partial变量（这会导致foreach出来的所有instance都具有私有、独立的partial。计算完毕后，调用reduce_add就可以将各个instance的partial给加起来）。
![](assets/uTools_1699238710685.png)


## Three Model

![](assets/uTools_1699250216958.png)

![](assets/uTools_1699252280741.png)

Use shared address space programming within a multi-core node of a cluster,use message passing between nodes

### Shared address space model

**Symmetric(shared-memory) multi-processor(SMP)**:
- Uniform memory access time: cost of accessing an uncached 
- memory address is the same for all processors
- **low scalability**

![400](assets/uTools_1699249111988.png)
![400](assets/uTools_1699249145780%201.png)


**Non-uniform memory access (NUMA)**: 
- All processors can access any memory location
- the cost of memory access (latency and/or bandwidth) is **different** for different processors
- **high scalability**
- 处理器旁边就有Memory，同时也能较高延迟地访问其他内存

![](assets/uTools_1699249008905.png)

NUMA可以在本地保存运行栈等其他处理器不需要的数据，同时在并发的时候其他处理器不处理的数据也能完全放在本地执行。

### Message passing model

两个线程独立运行，有自己的私有空间，互相之间唯一的交流方式是send/receive message。

MPI: message passing interface

不要求机器实现共享l/s功能，只需要能传递消息。可以用于大型机间传递信息。

### Data-parallel model

**Basic structure**: map a function onto a large collection of data
- Functional:side-effect free execution
- No communication among distinct function invocations (allow invocations to be scheduled in any order,including in parallel)

以前是用vector运算，但现在都是SPMD了。

ISPC中，编程可能会出现不确定性（并行的instance的结果写在同一个地方）。因此要使用更正规的编程方式来避免这些谬误。

functional form: **stream programing model**
- Streams:collections of elements. Elements can be processed independently.
- Kernels:side-effect-free functions. Operate element-wise on collections.

![](assets/uTools_1699250968532.png)
![](assets/uTools_1699251173848.png)

但有一个缺点是，中间这个tmp流在处理过程中不一定有必要，当过程太多的时候可能会需要创建极多的流，造成浪费。最好可以有某种方式把foo和bar整合起来。

- gather: 修改取值来源逻辑（将多地的数据给聚到一起）。
- scatter: 修改赋值目标逻辑（将数据放到buffer里然后分配到不同的地方）。

indices是索引流，表示数据位置，将input的按照索引重排列为tmp_input以load，或将output重排列为tmp_input以store，且不发生复制。
![](assets/uTools_1699251617976.png)

![](assets/uTools_1699251956629.png)


# Lec4 Parallel Programming Basics

## Create a parallel program

![](assets/uTools_1699324122730.png)

![](assets/uTools_1699324075283.png)

### Decomposition

要追求获得足够多足够小的task，以在多线程时可以通过调度来让整个机器保持忙碌。

因为Amdahl定律，一定要将大部分复杂任务全都通过并行加速，只留下简单的任务进行串行，否则加速比不会很大。

### Assignment

目标是：
1. 平衡工作压力
2. 减少通信成本

没用foreach的普通的ISPC（programmer-managed assignment）就是**static assignment**；但使用了foreach之后，具体调度交给运行时判断（system-managed assignment），因此是**dynamic assignment**。

ISPC也有一种task分配法，维护task数组，和一个下标，当有worker完成任务时，就会从task数组的下标处领取新任务，然后下标会自增。虽然这很好地实现了负载平衡，但队列本身需要大量的软件开销。这种方法叫**work queue**。


### Orchestration

Involve：
- Structuring communication
- Adding synchronization to preserve dependencies if necessary
- Organizing data structures in memory
- Scheduling tasks

Goals: reduce costs of communication/sync, preserve locality of data reference,reduce overhead,etc.

### Mapping

把worker（thread）分配给硬件单元。可以由os、编译器、硬件来分配。


## example: grid solver

要求把一个矩阵中每个点的上下左右一起求平均数作为当前点的新值，若所有点的新旧值之差的绝对值的平均数少于阈值，则结束程序。由于是图像处理算法因此不要求精确结果。如果全部并发跑的话，会导致数据竞争。

复制一份的方法不被接受，因为数据几乎被严禁复制一份。

一种分离依赖的方法是，同一个对角线上的点不会互相依赖。但这样长短不一非常难以调度，且会出现空间局部性的破坏。
![](assets/uTools_1699359365821.png)

将矩阵分为独立的两个部分，同一个部分内完全都可任意调度，红黑两阶段轮流进行。但也需要barrier来让两阶段分离。
![](assets/uTools_1699359669164.png)

所有thread完成当前阶段后，互相交流更新的结点（此处将各个线程的内存看做独立），然后才能切换阶段。
![](assets/uTools_1699360223258.png)

同一个阶段内的任务分配也有多种。而线程切割线旁边的点会在阶段切换时在不同的线程间交流。
![](assets/uTools_1699360486953.png)

### data-parallel

对红点（或黑点）条件使用for_all，让一个阶段的所有点可以被程序任意地并发调度，无需再理会细节。

### shared address space

需要注意barrier的实现。有三个barrier，作用分别是：
1. 确保对diff的重置完成前没有意外的写入。
2. 确保diff已经收集到所有的myDiff。
3. 防止领先的线程直接进入下个循环将diff置为0，导致后来的线程认为diff足够小，结果将done置为true了。
![](assets/uTools_1699362066334.png)

如果将diff设置为三个，每次只用其中一个，这样的话下一次循环的初始化会提前完成，与目前的读写完全互不干扰，从而可以减少两个barrier。如果只弄两个的话，有可能导致有领先的线程直接跑到了下一轮的初始化位置，把当前任务给初始化掉了。
![](assets/uTools_1699366497303.png)


### message passing

每个线程的内存私有性加强了，无法随意r/l数据了。矩阵被分为多份，且没有重复部分。

那么，为了让边缘点能够被计算，则需要互相发送数据。
![](assets/uTools_1699369900031.png)

但交换数据的一个问题是，阻塞式IO可能造成死锁，可能发送与接收无法匹配。因此，可以让奇数都发送，偶数都接收，完成之后再反过来；或者使用非阻塞IO。


# Lec5 Work Distribution and Scheduling (Performance Optimization P1)

目标：
1. balance workload
2. reduce communication
3. reduce extra work

TIP 1: Always implement the simplest solution first, then measure performance to determine if you need to do better.

static assignment要求任务的数量和执行时间是可预计的，从而能提前估计、分配。

dynamic assignment中，work queue方法被归类为**fine granularity partitioning**（1"task"=1 element）。这样可以非常好地负载均衡，但同步的开销很大（锁）。一种优化方式是Increasing task granularity，每次都同时领取多个任务，增加粒度，从而减少锁开销，但会降低负载均衡能力，因此需要权衡。

若work queue中每个task大小不一，那么，优先分配大任务是最好的，因为这样分配到最后的时候，各线程的任务差距就不会很大，不会发生到末尾的时候一个线程突然领了个特别大的任务的情况。

Distributed work queues：每个线程都有自己的work queue。刚开始时进行粗略的直接分配，当一个线程跑完自己的任务后，就可以从别人的队列steal任务。依然有锁逻辑，但由于大部分情况下只有thread自己在用锁，使得实际上锁的消耗几乎没有。不过steal的目标是谁、steal多少任务依旧是个问题。
![](assets/uTools_1699431707403.png)

当task之间存在依赖性时，在一个task的所有依赖被满足（如指定的若干个task全部完成）之前，不会离开队列，即不会分配给worker。

## Fork-join parallelism (Cilk Plus)

Fork-join parallelism: a natural way to express divide-and-conquer algorithms.

Cilk Plus: runtime implements spawn/sync abstraction with a locality-aware work stealing scheduler. 是对Cpp的一种拓展。

- `cilk_spawn`(fork): 加在函数调用的前面，使得该函数会异步执行（potential parallelism）（但究竟如何异步是运行时环境决定的）。
- `cilk_sync`(join): 阻塞，直到当前函数所有的`cilk_spawn`都结束。所有函数调用在return前都隐式执行`cilk_sync`。

多线程执行任务的时候，要注意不浪费主线程（第二种代码是不推荐的）：
![](assets/uTools_1699862473016.png)

并发快排时，我们应该在任务足够小时不再进行并发（PARALLEL_CUTOFF），而是串行执行。
![](assets/uTools_1699862697701.png)

具体实现中，若有太多太多的异步任务的话，全都并发执行是有极大副作用的。**parallel slack**: ratio of independent work to machine's parallel execution capability (in practice:~8 is a good ratio).

### child & continuation

```cpp
cilk_spawn foo();
bar();
cilk_sync;
```

- spawn child: `foo()`。
- continuation: `bar(); cilk_sync;`及之后的剩余部分。

上面的程序执行的时候，若采用child first策略，则当thread_0执行到`cilk_spawn foo();`时，则会开始执行foo，并且将continuation（bar）放入work queue；thread_1无事可做，就会从thread_0的work queue里窃取continuation去执行。
![](assets/uTools_1699863952912.png)

>Cilk选用child first。
### 循环

当continuation first时，在偷窃发生之前，会一直执行循环，直到把所有任务都放进queue里面。空间复杂度较高。类似BFS。执行顺序相比于串行会被打乱。
![](assets/uTools_1699864194130.png)

当child first时，本地会执行foo，然后将循环的条件放入work queue；其他线程窃取该循环条件（或者原线程执行完毕）后，才会继续执行新的任务。queue里面永远最多只有一个任务。类似DFS。
![](assets/uTools_1699864451046.png)

### 递归

分治法的时候，队列顶（上）到底（下）的任务规模会逐渐变小。显然偷走大的任务会提高效率，因此窃取策略是从顶部窃取。
![](assets/uTools_1699865345962.png)

### sync

当所有spawn结束后，最后谁来负责统计和继续执行也是个问题。

如果记录里面没有任何其他worker窃取过自己，则可以认为sync时不需要做任何事，自己做完了等于所有人都做完了。
#### stalling join policy

stalling join策略将一个数据结构放在thread_0里，其他线程spawn或done时会通知它，于是thread_0可以统计被spawn了多少次、done了多少个。全部完成后thread_0就可以直接知道sync已完成，继续执行。
![](assets/uTools_1699866359242.png)
![400](assets/uTools_1699866484255.png)


#### greedy policy

把数据结构放到work queue中。所有线程只要还空闲就会尝试窃取，并修改其spawn和done。谁窃取到时发现全部完成了，则这个线程自发结束sync，并往下继续执行。

>Cilk选用greedy policy。


# Recitation 1-ILP.SIMD instructions

乘法很耗时，除法更耗时。

尽量把loop中的重复计算给提取出来。

可以使用strace，来看看系统调用是不是频繁在申请内存，可以用于观察内存泄漏或不合理使用。

loop本身也开销极大，因为每次循环都需要检测一次条件，然而实际有意义的只有最后一次。循环展开（loop unrolling）可以缓解此问题，但循环展开后指令长度过长，使得instruction cache不够用，即空间局部性差。

编译器会做很多事情，如循环展开、SIMD，但某些情况下编译器无法认知到独立性，从而难以进行优化。当我们做最简实现的时候，可以查看汇编代码，看看编译器已经做了什么。

把能提取的都提取出来，甚至另起炉灶多来个for循环来储存中间值，这样会使得有更多变量可以设为uniform。

# Lec6 Graphic Processing Utils and CUDA

## graphics pipeline

![](assets/uTools_1700876868454.png)

- **Input, Vertex Generation**: a list of vertices in 3D space (and their connectivity into primitives). 根据点集，每三个vertices（vertex的复数）构成一个triangle

![](assets/uTools_1700876934211.png)

- **Step 1, Vertex Processing**: given a scene camera position, compute where the vertices lie on screen.

- **Step 2, Primitive Generation**: group vertices into primitives.

![300](assets/uTools_1700877209121.png)

**Step 3, Fragment Generation**: generate one fragment for each pixel a primitive overlaps.

![300](assets/uTools_1700877545971.png)

**Step 4, Fragment Processing**: compute color of primitive for each fragment(based on scene lighting and primitive material properties).

**Step 5, Pixel Operations**: put color of the "closest fragment"
to the camera in the output image.

![300](assets/uTools_1700877678688.png)

Vertex Processing 和 Fragment Processing都可以插入用户程序shader来改变渲染逻辑。


## CUDA

![](assets/uTools_1700879997658.png)

CUDA的thread抽象和p thread的thread很像，但二者的具体实现非常不同。

较高层面上，CUDA像是data parallel计算模型，每个thread对不同data进行计算。

### block

Grid是thread的点阵，每个thread被赋予一个三维的编号（下面例子是二维的），高维的编号更易进行问题的计算。若干的相邻thread会被划分为一个block，每个block也有对应维数的编号。

>高维是由CUDA支持的。

![400](assets/uTools_1701494137801.png)

使用CPU代码（**Host代码**）对待处理的问题进行划分。如矩阵相加，根据块大小来划分任务，然后使用**thread launch**（`<<< ... >>>`）调用GPU代码（**Device代码**）。GPU代码（kernel function，使用`__global__`声明）可以通过block编号以及block中线程的编号来计算得到对应线程所负责的任务。

![](assets/uTools_1701494057393.png)

block对硬件来说是virtual的，是CUDA语言的一种抽象，但实际上反映出其对GPU的调用方式。Block大小不能超过1024。

>GPU丢弃了CPU那些让单核运行更快的技术，如分支预测、乱序执行、大量cache，把这些丢弃后更容易进行高并发的各种简单计算，能耗低。

![](assets/uTools_1701495059439.png)

有可能矩阵尺寸并不是block大小的整数倍，因此有些非法数据。此时只要**不对外进行赋值即可**，内部计算了也不会有事：
![300](assets/uTools_1701495394340.png)

>Block的思想结构能帮助我们的对其的编程。

CUDA的模型是SPMD（Single Program Multiple Data），即单一的kernel code被多个thread调用。

block的数量是无限的，就好像GPU有无限的单元一样，这给予了我们非常纯粹的data parallel抽象。但实际上的调用并不是这样的，GPU是有限的。

### memory

CPU和GPU有不同的地址空间。再加上并发模型不同，因此很难进行shared memory。
![](assets/uTools_1701496365520.png)

如果device要访问host的数据，则需要显式地进行`cudaMemcpy`。
![](assets/uTools_1701496487497.png)

>从`cudaMalloc`可以看出新式的c库函数风格：传入一个指针作为结果放置的位置，而函数的返回值用于指示是否调用成功。

CUDA的内存有三种：
1. 公共内存 **Device global memory**: 全员可访问。
2. block私有内存 **Per-block shared memory**: block内所有线程可访问。
3. 线程私有内存 **Per-thread private memory**: 单线程自己可访问。

三层的内存机制形成了**software cache**，可以通过程序指定哪些数据放在哪里。
![](assets/uTools_1701497087940.png)

### 例子

卷积操作，将相邻的三个元素加到一起。

>所有在函数内直接定义的变量都是per-thread private memory。

下面代码的输入和输出都来自于device global memory，没有利用cache，效率低。
![](assets/uTools_1701497838692.png)

可以使用`__shared__`来修饰变量的定义，以在per-block shared memory上创建变量。

block内的每个线程都负责从global载入一个元素到block中（要额外加载两个）。使用`__syncthreads()`来确保加载完毕，之后就能自由使用support数组了。

![](assets/uTools_1701498335696.png)

### synchronization

![](assets/uTools_1701500180762.png)

各个block类似于ISPC的for_all，即可以以任意顺序调度。但thread之间有同步操作，有较强的数据相关（使用per-block shared memory），因此不太能任意调度。
### CUDA abstractions

![](assets/uTools_1701499456607.png)
### assignment

block数量可以是无限的，但GPU资源是有限的，因此GPU需要进行assignment。

CUDA将每个block（被视为一个work）分配给每个GPU计算单元，使用动态分配。

![](assets/uTools_1701499998415.png)

### NVIDIA与Warp

2014年的NVIDIA GTX 980使用了Warp技术，将任务状态保存在各个warp中，每次都会调用可执行的任务放到ALU中去执行（一个thread如果在读取内存的话，可能就会切换另一个warp去执行）。

![](assets/uTools_1701501335552.png)

warp并没有在CUDA中被抽象，它在程序上透明，但针对warp的性质进行编程能大幅提高效率。

（抄自知乎）以下是关于 warp 的一些关键点：
1. **同步执行**: 在一个 warp 中的所有线程都会同时执行相同的指令，这种机制也被称为 SIMD（单指令多数据）或 SIMT（单指令多线程）模型。
2. **线程调度**: GPU 的调度单元以 warp 为单位进行调度，而不是单个线程。这意味着整个 warp 会被分配到一个流多处理器（SM）上并一起执行。
3. **分支处理**: 如果 warp 中的所有线程都采取相同的分支路径（例如，都满足某个条件语句），则它们会继续同步执行。但是，如果线程在分支上有不同的路径（即分歧），则 warp 会执行每个路径，但不是所有线程都会在每个路径上活跃。这可能导致效率下降，因为即使某些线程在特定路径上没有工作，整个 warp 也必须等待该路径完成。
4. **性能优化**: 了解 warp 的行为对于优化 CUDA 程序的性能至关重要。例如，为了确保高效执行，开发人员可能需要确保他们的代码减少 warp 分歧。
5. **占用率**: 在 CUDA 中，占用率是一个重要的性能指标，表示每个 SM 上激活的 warps 与 SM 可以支持的最大 warp 数量的比例。更高的占用率通常意味着更好的硬件利用率。

因此似乎把每个warp看成大型的向量计算比较好。
# Recitation 2-CUDA programming 1

## 术语和基本语法

![](assets/Pasted%20image%2020240129103744.png)

![](assets/uTools_1706495897520.png)

![](assets/uTools_1706495611555.png)

![](assets/uTools_1706495628173.png)

![](assets/uTools_1706496720540.png)

![](assets/uTools_1706497397285.png)

![](assets/uTools_1706497968668.png)

调试工具：
![](assets/uTools_1706498388237.png)

## Advice

![](assets/uTools_1706498608931.png)

![](assets/uTools_1706499869139.png)

![](assets/uTools_1706500219705.png)

## SPMD model

SPMD只有barrier进行同步。

![](assets/uTools_1706502089582.png)

![](assets/Pasted%20image%2020240129122544.png)

Long tail problem: 有少数任务过长导致其他任务等待。

![](assets/uTools_1706502565366.png)

![](assets/uTools_1706503088718.png)

![](assets/uTools_1706503461805.png)

## CUDA Program

![](assets/uTools_1706503992714.png)

下面的二维编程相当于把block按二维坐标顺序排好，就可以得到一个大的thread方阵，其尺寸和矩阵C的尺寸一样。这样就能让一个thread对应C的一个点了。**block和thread的编号是横x竖y，以左上角为起点**。
![](assets/uTools_1706504186791.png)

32是warp size，因此每个block的thread数必须是32的整数。
![](assets/uTools_1706504932980.png)

>严格意义上说，warp是一堆指令，而数据存在对应的内存中，但我们依然可以让数据跟着warp走。

>host可以持有指向device内存的指针（很合理），但不能对目标内存进行读写。

### Inverted Indexing

将x和y反过来用得到inverted写法，但会导致效率慢十倍。
![](assets/uTools_1706505546699.png)

每个warp的y固定、x连续，需要追求在一个warp内达到最高效率。影响效率是不是cache，而是读写速度，如果读写不连续就不得不使用gather和scatter。

![](assets/uTools_1706506025090.png)

通过“y固定，x连续”的规则，结合向量计算的性质（如循环变量在任意的一个瞬间在每个thread中相等），就可以推导出一个warp中各个变量的性质，然后再做分析。

warp中每个thread的k在某个瞬间里都是相等的话，那么一个warp里所有thread读$(k,j)$的时候，就是读一段连续内存。而$(i,k)$中的i是全warp的thread固定使用的。

![](assets/uTools_1706506777368.png)

翻转后，由于warp中每个thread在一个瞬间要使用不同的i，并且$(i,k)$分散在不同的地方，就需要gather。j都相同，因此$(k,j)$固定，但$(i,j)$和$(i,k)$一样分散，需要scatter。

![](assets/uTools_1706507376227.png)

另一个看待inverted逻辑的角度是，它的每个warp之间有过多的重复内存使用，可以看成一种memory conflict。

如果提前转置第二个矩阵的话，其效率和inverted几乎一样。因为乘法里面的$(j,k)$出现了gather，而scatter以及转置的gather只执行一次因此影响小。**二者宏观上都可以看做单纯进行了多次的gather**。
![](assets/uTools_1706509827584.png)

![](assets/uTools_1706510586342.png)

## GPU Hierarchy

warp内部是lockstep的，它们像向量运算一样全部步调一致地执行指令，也因此完全不存在同步问题。
![](assets/uTools_1706510766887.png)

## Programming with Blocks

在单block中的运算可以使用`__shared__`定义的变量，本地性更好。

![](assets/uTools_1706510978511.png)

![](assets/uTools_1706511211909.png)

![](assets/uTools_1706511333285.png)

![](assets/uTools_1706511672890.png)

![](assets/uTools_1706511903138.png)

![](assets/uTools_1706512151462.png)

所有thread先负责获取一个变量到shared memory，那么整个block都执行完获取后（需要同步，因为不是单warp），就能获取到所有等一下需要的变量。然后就可以随意计算了。

![](assets/uTools_1706512359123.png)

![](assets/uTools_1706512633160.png)

使用barrier的时候要小心，不能随意直接跳过barrier同步代码。下面这段代码在触发continue之后会直接跳过`__syncthreads()`，这就导致该barrier永远无法等到所有thread到达（根据具体情况，程序可能卡在不同的时机）。
![](assets/uTools_1706513334659.png)

**计算负荷<<内存负荷**，因此要首先确保内存速度做到位，为了内存的优化带来的计算消耗几乎都是可以忽略不计的。

![](assets/uTools_1706513889355.png)

有一种加速是更好地利用内存总线（应该是指总线宽度，每次都读取满满当当的数据），来加大内存吞吐量：
![](assets/uTools_1706514333732.png)








