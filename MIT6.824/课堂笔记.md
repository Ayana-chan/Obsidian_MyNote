# 链接
[6.824 Home Page: Spring 2020](http://nil.csail.mit.edu/6.824/2020/)

[〖项目笔记〗分布式架构详解 - 哔哩哔哩](https://www.bilibili.com/read/cv19361441)
# 基本概念

- Key-Value = KV: 目前的理论、开发都是基于KV系统的，输入都是Map操作。
- fault tolerance: 容错
	- availability: 即使是很少发生的小错误，在一个集群里面可能天天发生。要保证一定范围内的错误发生时依旧可用。
	- recoverability: 可以通过恢复系统解决故障，也可以在失去可用性后重新可用。
- consistency: 一致性
	- non-volatile storage (NV storage): 非易失性存储
	- replication: 拷贝（副本）

# MapReduce

输入调用Map（emit(k,v)），产出中间输出，然后调用Reduce（emit(k,list(v))）（value以数组形式聚合到一句Reduce中）以实际生成数据。

>有一个经典的例子就是word-counter，我们输入的是若干篇文章，也就是`list(k1, v1)`，其中`k1`就是文章的filename，`v`就是content。然后，master会将所有的文章分成M份，分别派发给这么多的worker去做（不一定是同一时刻）。那么worker执行map的时候，就会把每个`(k1, v1)`都去执行map-function（用户定义的），然后得到的结果是`(k2, v2)`，这里的`k2`就是word，`v2`就是word出现的次数。然后，将所有的`(k2, v2)`对分成R份，存在磁盘中。那么，等所有的map都做完后，其实是有M\*R个文件的。后面reduce过程就会将其reduce id对应的所有map result收集起来，因为一个word会在不同的map result中出现，所以我们整理会得到一个`(k2, list(v2))`，再对每一个这样的pair调用reduce-function，就会得到`(k2, v2)`，也就是把同一个单词出现的次数都加起来了。然后存在结果中就好了，等待merge。

![](assets/uTools_1679798365904.png)

![](assets/uTools_1679754326785.png)

 - intermediate output: 中间输出 每个Map以一组KV作为输出。
 - job / task: 整个计算成为job，而每次调用MapReduce都是task。
 - GFS = Google File System: 这种文件系统在收到大文件的时候会将其自动拆分并均匀分配到所有GFS中。
 - shuffle: 洗牌，指行Map转为行Reduce。
 - RPC = Remote Procedure Call: 远程过程调用。直接调用其他进程（机器）上的方法。go的RPC包很好用。
 - parallelism: 并行化
 - coordination: 协作。线程间同步（如等待其他线程完成任务）。

# GFS

高性能和低异常不可兼得：

| From        | To              | 备注   |
| ----------- | --------------- | ----- |
| Performance | Sharding        | 通过分片提高性能，但分片太多总有某些服务器会出错导致数据出错 |
| Faults      | Tolerance       | 使用容错机制解决错误     |
| Tolerance   | Replication     | 最简单的容错就是复制       |
| Replication | Inconsistency   | 复制可能带来不一致性问题  |
| Consistency | Low Performance | 想要保持一致性需要很多努力 | 

- sharding: 分片
- anomalous behavior: 异常行为
- replication/replica (REPL): 复制
- volatile: 易失的

朴素的replication有一个问题是无法保证写操作执行顺序相同：

![400](assets/uTools_1679996515140.png)

GFS特点：
- 只有单个机房（数据中心）
- 只能内部使用，不公开
- 只处理大文件的顺序访问（牺牲延时换取吞吐量）
- 不需要很强的一致性，web相关信息允许一些错误

Master Datas（注：v表示易失，存在内存里面，nv存在硬盘里面）:
- FileName
	- Array of chunk handles: nv
- Handle（对应一个chunk；根据handle可以找到chunk在哪个chunkserver）:
	- List of chunkservers: v
	- version: nv
	- primary: v
	- lease expiration (租期): v
- Log & CheckPoint -> Disk

每个chunk一般有三个备份，一个primary两个secondary。

使用log而非数据库的理由：数据库的索引会导致随机存取，而log只需要找EOF。

CheckPoint会挑时间保存**master**的完整快照（而不是log），可以加快master的重启，只要加载checkpoint然后重新执行checkpoint之后的日志就行了。

之所以version必须是非易失的，是因为如果某个chunkserver宕机了一段时间，其版本号就会匹配不上，则请求数据的时候不会去匹配这个chunk。

master重启时会询问所有的chunkserver（平时也会定期询问），来确定handle。若把version选为这些chunkserver中最新的那个，有一个风险是，持有最新版本的chunkserver在master重启时无法回应，导致整个系统只能使用旧版本。

master若发现现有的chunkserver都是旧版本的，则会一直等待直到对应版本被发现。

若发现有拥有比version更高的版本号的chunk，则master会认为在分配primary时出现了错误，于是重新选择版本号。

Brain Split （脑裂）问题：一个primary正在工作，但由于网络问题master联系不到primary，结果误新指定了个primary导致同时存在两个primary。虽然master会告诉client新的primary，但client可能还在和旧primary正常通信。解决方法就是租期，master知道什么时候租约到期。当master联系不上primary时，就等待租约到期后再重新指定即可。

添加新文件（没有任何chunk与之相关）时，master会新建各种信息、随机指定P和S并进行特殊的创建工作。
## Read

应用想读取一个文件的字节偏移a到b范围内的数据，则把文件名和始终偏移量传给master，由master来找文件名、寻找chunk。

客户端能算得出来自己要读的文件在第几个chunk、要几个chunk，但它不知道chunkserver是哪个。

## Write

（对某个chunk）<u>如果没有primary</u>，master寻找到最新版本，并且决定哪个是primary、哪个是secondary。之后自增版本号，并通知这些P、S。此时会定下租期，告诉P这个primary身份的期限。但版本号的持久化时机老师并没有确切回答。

>只有在无primary时才改变版本号，同时决定P和S。

会有多个client发送请求，而P可以决定执行顺序，不需要S再去调度。

master告诉client哪个是primary。primary接收client的信息，并选择一个偏移量（要保证能存的下）；同时通知其他secondary该偏移量。这样，所有replica就存在了同一个地方。

S会告诉P成功or失败。有任何一个失败就会通知client操作失败，要求client重新走流程（当做上个请求完全没发生）。因此，即使失败，也会有某些cs执行了该写操作，导致能不能读到的随机的。但是，client知道失败后，自然会重新发起请求直到写入成功。只要操作进行到底，就能保证全部写入成功。但每次请求的偏移量都不同，所以之前的偏移量的地方有没有该记录也是随机的。换句话说，保证最终必然有一块一方存储了这个写入数据。

![400](assets/uTools_1680065941129.png)

似乎一般都是加写数据，写在chunk中已存储的最后位置之后。

如果一个文件的顺序很重要，比如电影文件，那就应该连续一次性写完，而不是分多次写。

# Primary-Backup Replication

只能解决fail-stop faults，而不能解决bug。

解决方法：
- State Transfer 复制状态（类似快照）（不关心是否并行）
- Replicated State Machine 传递外部事件（类似日志）（不能用于新建replica）

P/B sync: Backup往往比Primary慢，当P宕机时，B不一定能赶得上进度。

Non-determined Event: 例如生成随机数和获取当前时间，不能交给主从机两个机子做。从机必须拿主机的计算结果。

B只有在知道自己什么时候能停下时，才会开始执行。使用buffer缓存这些指令。

主机要回复client时，等从机接收client的指令并发来ACK后主机才真正回复。

假如出现一种情况，主从先后发了同一个回复给client，也不会出问题。因为回复的时刻主从状态完全相同，则网络包的各种东西都完全相同，在协议里面被视为重传而丢弃。各种方案几乎都可能产生这样的重复输出。

Test and Set Server: 具有一把锁决定哪个机器能上线（成为P）

# Raft

[分布式算法：Raft论文翻译 - 知乎](https://zhuanlan.zhihu.com/p/343560811)

任何时候，服务器是领导者、追随者和候选人三个中的一种状态。在正常操作中，只有一个leader，而其他所有服务器都是follower。

- follower是被动的：他们自己不发出请求，而只是响应leader和candidate的请求。
- leader处理所有客户请求（如果客户联系follower，则follower将其重定向到leader）。 
- candidate，用于选举新的领导者。

![](assets/uTools_1680235452963.png)

每个任期(term)开始一次选举(election)，其中一个或者更多的candidate试图成为leader。 如果candidate在选举中获胜，则在该任期剩余时间中将担任leader。 在某些情况下，选举将导致投票分裂。 在这种情况下，任期将以没有leader的情况结束；新的任期(新的选举)会很短。Raft保证一个任期内最多只有一个leader。

所有人都选自己以及比自己新的，则每一个被选为leader的服务器所包含的日志肯定被复制到了超半数的服务器上，也就都是已提交的日志。（但某一日志即使被复制超半数也不一定是已提交，这在后面会提到）

![](assets/uTools_1680235477123.png)

Raft使用心跳机制来触发leader的选举过程。当服务器开始工作，它们成为follower。只要服务器收到来自leader或者candidate有效RPC，他就会保持follower状态。leader向所有follower发送周期性的心跳信号（不携带日志条目的AppendEntries RPC），以维护其领导地位。如果follower在称为选举超时(election timeout)的时间段内没有任何通信，则它假定没有可行的leader，并开始竞选新leader。

- split votes: 分裂投票

- RequestVote RPC 由Candidate在选举期间启动
- AppendEntries RPC 由Leader启动用来复制日志条目并提供一种形式的心跳

当等待投票的时候，candidate可能收到另一个服务器声明已经成为leader的AppendEntries RPC。如果这个leader的任期不低于这个candidate的任期，这个candidate就承认leader的正当性然后成为follower。如果这个RPC中的任期比candidate的任期小，这个candidate就就拒绝这个RPC，然后继续保持candidate状态。

>If election timeout elapses without receiving AppendEntries RPC from current leader or granting vote to candidate: convert to candidate

每个服务器的选举超时时间都是随机的（大于心跳周期，比心跳周期大几倍就相当于容许丢失几个心跳周期）。

投票后也会重置定时器。

如果发送和接收的任何RPC中的对方任期比自己任期大，则**自己变为follower且更新自己的任期**。
## 日志

日志由条目组成，这些条目按顺序编号。 每个条目包含创建它的任期（每个框中的数字）和状态机指令。 如果可以安全地将该条目应用于状态机，则认为该条目被提交。每个日志条目还有一个数字索引表示它在日志中的位置。

![](assets/uTools_1680242280464.png)

条目的提交会把以前的所有条目给提交，因为几条记录的是最高索引。该索引会包含在AppendEntries RPC中使得follower都知道哪些条目被提交了。**若得知该条目被提交，则可将其正式应用与本地状态机。**

如果两个在不同日志的条目有相同的索引和任期，那么它们储存同样的指令，且这两个日志中该索引之前的条目都是相同的

发送AppendEntries RPC时，leader在其中包括其日志中下一条新条目之前的索引和任期(当前已提交的最大索引和任期)。 如果follower在其日志中找不到具有相同索引和任期的条目，它拒绝该条目。 一致性检查是一个归纳步骤：日志的初始空白状态满足LogMatching属性，并且每次扩展日志时，一致性检查都会维持Log Matching属性。结果，只要AppendEntries成功返回，leader就会知道follower的日志和自己的新条目之前的日志完全相同。

>在Raft中，leader通过强制follower复制leader的日志来解决不一致问题。这意味着follower中冲突的条目会被leader的日志覆盖。leader会找到两个日志中相同的最新日志条目，然后删除follower日志中该条目之后的所有内容，然后将leader中该条目之后的所有条目发给follower。 所有这些操作用来响应AppendEntriesRPC执行的一致性检查。 
>leader为每个follower维护一个nextIndex，这是leader将发送给该follower的下一个日志条目的索引。当leader刚选举出来时，它将所有nextIndex值初始化为其日志中的最大索引之后的索引（图7中的11）。 如果follower的日志与leader的日志不一致，则下一个AppendEntries RPC中的一致性检查将失败。失败之后，leader递减nextIndex并重试AppendEntries RPC。 最终nextIndex将到达leader和follower日志匹配的点。至此，AppendEntries将成功执行，这将删除follower日志中的所有冲突条目，并从leader的日志中添加附加条目（如果有）。 一旦AppendEntries成功，follower的日志将与leader的日志保持一致，并且在本任期的其余部分中将保持这种状态。

## 应用日志

过半的服务器响应AppendEntries时，leader（S1）回应客户端“提交成功”，并让各个服务器将其应用到本地（也许是在下一个AE中）。

![](assets/uTools_1681967002882.png)

## 快速备份

当leader发现一个follower日志不匹配时，会一项一项地往前试知道发现匹配项。如果一台服务器宕机了很久，那就得一直回滚非常多次，浪费时间。

- XTerm: 冲突项的任期
- XIndex: 冲突项所在任期的第一个项的下标
- XLen: 空白的长度（prev的index与当前日志的最大index之差）

- Leader拿到XTerm，然后找到自己的**XTerm的日志**的**最后一项**的下一项（**大于Xterm的日志**的**第一项**），将其设为nextIndex。
- 若Leader的日志里面没有该Xterm，则将nextIndex设为**XIndex**（即把XTerm全部覆盖掉）。
- Follower没有该prev下标时，返回XTerm为-1，XLen为下标差。Leader将nextIndex减去XLen。

第二种情况有一个错误的想法是让其和第一种统一，即把nextIndex设为**大于Xterm的日志**的**第一项**。考虑如下情况（下标从1开始）：

- F: 3 4 5 5 5 5
- L: 3 4 4 4 6 6 6

若使用该错误想法的话，nextIndex退到5就无法再退了。退到3才是正确的。

## 安全性

这个算法让**上任leader所有提交的条目在新leader中都存在**（所有已提交的条目都会被leader包含，也就是说**没有被提交后又被废弃的条目**）。而被提交的条目才会被服务器正式应用。

日志条目只有一个流向，从leader到follower，而且<u>leader从不修改它已存在的日志</u>。

>RequestVote RPC 包含有关candidate日志的信息。如果follower自己的日志比candidate的日志新，则拒绝投票。

>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新(up-to-date)。 如果日志中的最后一个条目具有不同的任期，则带有**较新任期**的日志将是最新的。 如果日志以相同的任期结尾，则**更大索引**的日志是最新的。

一个条目即使被复制到了大多数服务器，也不一定是已提交的。若还在复制但未达到提交就宕机、重启后又有新的输入的话，**旧的输入复制再多也不算提交**，只有最新的输入被复制到过半服务器了才算提交（因为这样的话就不会投票给其他leader导致新旧输入都被覆盖）。

![](assets/uTools_1680247505548.png)

如果服务器S1添加（收到AppendEntries）了S2的条目，而S3没有添加S2的条目，此时S1不可能投票（收到RequestVote）给S3。

## 持久化

要持久化log、currentTerm、votedFor。

要存当前任期和投票给谁是为了防止服务器在一个选举期之内宕机又恢复导致一个人投两票。

要存任期是为了防止所有服务重启后不知道当前进行到哪一个任期，随意选择的话可能和其他日志冲突。

## 线性一致性

Linearizability: 线性一致性，等同于强一致性（strong consistency）。在该lab里面等价于正确性。

- 历史记录中的操作具有顺序（意味着可以画出箭头来标明各个操作的开始与结束之间的时序关联）
- 这些操作的顺序与非并发的请求现实时序是匹配的（现实中的先后也构成箭头）
- 每次读取都能读到最近一次写操作的结果（规定了某个读必须在某个写后面，要画上箭头）

该图内每个操作的做竖线表示发起请求，右竖线表示收到确认。叉叉表示服务器操作这片内存的时间点。箭头表示的是叉叉之间的关系。

当画好箭头后出现环，则说明不是线性一致的。

![](assets/uTools_1687606732615.png)

![](assets/uTools_1687607479063.png)

多副本很容易打破线性一致性，因为很可能出现乱序请求或过时信息。这也是为什么它也叫强一致性。良好的线性一致性使得分布式系统表现得就像一个单线程系统一样。

## 请求响应过迟问题

即使满足了线性一致性，如果一个请求的响应时间太迟，就可能让应用程序读到过期的数据。此时，应用程序应当包含对此现象的容错机制。

# ZooKeeper

## 分布式部分

Raft不是一个可以直接通信的独立服务，而是一个库，必须包含在具体的程序中被显式地调用。ZooKeeper的目的就是构建一个人人可用的独立服务。

问题：
1. API: general-purpose coordination service （一个人人可用的独立服务的API应该长什么样）
2. N台服务器能提供N倍的效益吗——由于读可以发给replica，因此可以趋向正相关（Raft是反相关，因为都是leader干活）

ZK允许把只读请求发送给replica，得到的回复有可能是过期（stale）的而非新鲜（fresh）的。

ZK的保证：
1. Linearizability writes 线性一致性写（不符合线性一致性）
2. FIFO client order 
	1. 写的顺序是client specifified order，即client可以指定其发送的请求的处理顺序。
	2. 读时，虽然可能会有过期数据，但保证每次读都不会比上次旧。日志都会有版本编号zxid，client在读时会记录zxid，并在读请求时携带；replica只有当自己的日志更新到比zxid新时，才会去回复这个读请求。因此，读都是随时间往前的，或是说是沿日志号往前的。
	3. 写时也可记录写的日志的zxid，在之后的读就要求比此zxid新才行。这样的话，就保证对于一个client来说，必能读到其自己之前写的东西。

读的时候可以开启watch，若对应部分被更改，将其设为watch的client会收到通知。这样可以防止分多次读时部分过期的问题。replica会维护watch table来记录被watch的部分。但replica崩溃时并不会转移watch table，而是告诉client它崩溃了，让client做好相关应对。

## API部分

每次操作都会递增对应文件的版本号。操作可以指定版本号，对不上版本就不会进行操作并返回错误。

- CREATE(PATH,DATA,FLAGs): 创建文件，必须是唯一的，让客户端知道它是唯一管理这个文件的客户端（exclusive）。FLAGs决定其用途（形式）。
- DELETE(PATH,V): 删除文件。可以指定版本。
- EXIST(PATH,WATCH): 查询该文件是否存在。若设置了WATCH，则ZK会在该文件被删除/创造时通知客户端。
- GETDATA(PATH,WATCH): 获取文件内容。WATCH会监听文件内容。
- SETDATA(PATH,DATA,V): 修改文件内容。可以指定版本。

### 例1： count

需求：多个客户端都要求对一个变量做自增操作。

```python
while true
 x,V = GETDATA("f")
 if SETDATA("f",x+1,V)
	 break
```

解释：如果GET和SET期间有其他客户端完成了自增，或者GET的replica过时，则V会不对，导致要重新走一遍流程，直到执行期间没有任何其他客户端完成自增。

一旦成功，即是一次原子读写。也称为mini transaction。

若有n个客户端，则完成这件事的复杂度为O(n^2)，因此不适合大量用户，更不适合大型数据。因此ZK通常用于管理配置而不是存储大数据。

>这种有个目标就一呼而上、最后只有一个客户端能成功的现象，叫做**羊群效应（Herd Effect）**。

### 例2： 获取锁

需求：建立互斥锁。

多个客户端抢着创建一个文件，这个文件就被当做锁来用，删除文件就是释放锁。

>开启ephemeral后，客户端死亡时会自动删除文件。

```python
1 if CREATE("f" , ephemeral=T) return #创建成功就结束
2 if EXISTS("f" , watch=T)
3     wait #等待直到watch触发，即文件被删除
4 goto 1 #重新尝试创建文件
```


### 例3： 通过文件序列来管理锁

需求：建立一个无羊群效应的互斥锁。

>开启sequential时，ZK会在创建文件后加上后缀编号，而且只会不断变大，不会重用。

数个客户端抢着创建带后缀编号的文件f，则可以自动排序（数字越小的抢的越快），每个客户端都只有在比它们编号小的文件都删除后（先抢到的客户端都不再要锁后）才能算获得锁。

```python
1 CREATE("f",data,sequential=T,ephemeral=T) #创建文件并获取对应编号
2 LIST "f*" #列出文件列表
3 if no lower file return #没有比自己编号小的文件了，说明得到了锁
4 if EXISTS(next lower f , watch = T) #只需要监听最接近自己的编号小的文件
5     wait #等待直到目标文件被删除
6 goto 2 #重新检测是否获得锁
```

如果有文件f1、f2、f4、f6，则拥有f6的客户端只要不断检测f4是否存在，因为正常情况下锁都是**从小到大被一个一个释放**。

之所以要每次循环都重新列出文件列表、并且最接近自己的编号小的文件被删除后也不等价于获得锁，是因为**有可能客户端提前挂了或者不想再要锁了，导致其文件被提前删除**，如f1、f2、f4、f6直接变成f1、f2、f6，此时f6需要转而监听f2而非f4。
# CRAQ

**CRAQ**(Chain Replication with Apportioned Queries: 读均摊的链式复制)

假如有五台服务器，对于写操作，Client发给S1，S1做完后发给S2，一直发到S5，然后S5发给Client说“提交完毕”。S1称为Head，S5称为Tail。对于读操作，CR只能从Tail读。因此，**读到的东西都是被提交了的**。

保证线性一致性。

![600](assets/uTools_1687664446211.png)

![400](assets/uTools_1687664507467.png)

CRAQ的读可以给各个服务器读，但也不会读脏数据：

![](assets/uTools_1687664569716.png)


**故障**发生于写请求的传递过程。

- Head故障了，则直接让下一个结点当Head即可。
- 中间节点故障了，则将其踢出，并把请求重新发给下一个结点。
- Tail故障了，则直接用前一个结点当Tail即可，因为对中间节点故障的处理使得Tail和其前一个结点的状态是一致的，就差了这个新请求；Tail故障后，新Tail就把这个新请求提交。如果此时不提交的话可能面临重复请求等问题。


**性能**：Leader只需要向一个服务器发请求，而不像Raft一样全都要发，减少了Leader的压力。但问题是，如果有一个服务器变慢了，就会拖累整个系统，Raft就没有这种问题。

**网络割裂导致脑裂**：如果head结点到其他结点的网络断了，则head认为其他结点都宕机，自己一人挑起全家；下一个结点认为head故障了，把head踢掉自己当，并且通知Client要向自己发请求。于是产生了**脑裂**问题。此时需要配置服务器等第三方手段来记录存活情况，所有节点都得听配置服务器的，它说结点死了就是死了。配置服务器通常由Raft、Paxos、ZK等算法实现。





