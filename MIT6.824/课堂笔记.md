# 链接
[6.824 Home Page: Spring 2020](http://nil.csail.mit.edu/6.824/2020/)

# 基本概念

- Key-Value = KV: 目前的理论、开发都是基于KV系统的，输入都是Map操作。
- fault tolerance: 容错
	- availability: 即使是很少发生的小错误，在一个集群里面可能天天发生。要保证一定范围内的错误发生时依旧可用。
	- recoverability: 可以通过恢复系统解决故障，也可以在失去可用性后重新可用。
- consistency: 一致性
	- non-volatile storage (NV storage): 非易失性存储
	- replication: 拷贝（副本）

# MapReduce

输入调用Map（emit(k,v)），产出中间输出，然后调用Reduce（emit(k,list(v))）（value以数组形式聚合到一句Reduce中）以实际生成数据。

>有一个经典的例子就是word-counter，我们输入的是若干篇文章，也就是`list(k1, v1)`，其中`k1`就是文章的filename，`v`就是content。然后，master会将所有的文章分成M份，分别派发给这么多的worker去做（不一定是同一时刻）。那么worker执行map的时候，就会把每个`(k1, v1)`都去执行map-function（用户定义的），然后得到的结果是`(k2, v2)`，这里的`k2`就是word，`v2`就是word出现的次数。然后，将所有的`(k2, v2)`对分成R份，存在磁盘中。那么，等所有的map都做完后，其实是有M\*R个文件的。后面reduce过程就会将其reduce id对应的所有map result收集起来，因为一个word会在不同的map result中出现，所以我们整理会得到一个`(k2, list(v2))`，再对每一个这样的pair调用reduce-function，就会得到`(k2, v2)`，也就是把同一个单词出现的次数都加起来了。然后存在结果中就好了，等待merge。

![](assets/uTools_1679798365904.png)

![](assets/uTools_1679754326785.png)

 - intermediate output: 中间输出 每个Map以一组KV作为输出。
 - job / task: 整个计算成为job，而每次调用MapReduce都是task。
 - GFS = Google File System: 这种文件系统在收到大文件的时候会将其自动拆分并均匀分配到所有GFS中。
 - shuffle: 洗牌，指行Map转为行Reduce。
 - RPC = Remote Procedure Call: 远程过程调用。直接调用其他进程（机器）上的方法。go的RPC包很好用。
 - parallelism: 并行化
 - coordination: 协作。线程间同步（如等待其他线程完成任务）。

# GFS

高性能和低异常不可兼得：
| From        | To              | 备注                                                         |
| ----------- | --------------- | ------------------------------------------------------------ |
| Performance | Sharding        | 通过分片提高性能，但分片太多总有某些服务器会出错导致数据出错 |
| Faults      | Tolerance       | 使用容错机制解决错误                                         |
| Tolerance   | Replication     | 最简单的容错就是复制                                         |
| Replication | Inconsistency   | 复制可能带来不一致性问题                                     |
| Consistency | Low Performance | 想要保持一致性需要很多努力                                   | 

- sharding: 分片
- anomalous behavior: 异常行为
- replication/replica (REPL): 复制
- volatile: 易失的

朴素的replication有一个问题是无法保证写操作执行顺序相同：

![400](assets/uTools_1679996515140.png)

GFS特点：
- 只有单个机房（数据中心）
- 只能内部使用，不公开
- 只处理大文件的顺序访问（牺牲延时换取吞吐量）
- 不需要很强的一致性，web相关信息允许一些错误

Master Datas（注：v表示易失，存在内存里面，nv存在硬盘里面）:
- FileName
	- Array of chunk handles: nv
- Handle（对应一个chunk；根据handle可以找到chunk在哪个chunkserver）:
	- List of chunkservers: v
	- version: nv
	- primary: v
	- lease expiration (租期): v
- Log & CheckPoint -> Disk

每个chunk一般有三个备份，一个primary两个secondary。

使用log而非数据库的理由：数据库的索引会导致随机存取，而log只需要找EOF。

CheckPoint会挑时间保存**master**的完整快照（而不是log），可以加快master的重启，只要加载checkpoint然后重新执行checkpoint之后的日志就行了。

之所以version必须是非易失的，是因为如果某个chunkserver宕机了一段时间，其版本号就会匹配不上，则请求数据的时候不会去匹配这个chunk。

master重启时会询问所有的chunkserver（平时也会定期询问），来确定handle。若把version选为这些chunkserver中最新的那个，有一个风险是，持有最新版本的chunkserver在master重启时无法回应，导致整个系统只能使用旧版本。

master若发现现有的chunkserver都是旧版本的，则会一直等待直到对应版本被发现。

若发现有拥有比version更高的版本号的chunk，则master会认为在分配primary时出现了错误，于是重新选择版本号。

Brain Split （脑裂）问题：一个primary正在工作，但由于网络问题master联系不到primary，结果误新指定了个primary导致同时存在两个primary。虽然master会告诉client新的primary，但client可能还在和旧primary正常通信。解决方法就是租期，master知道什么时候租约到期。当master联系不上primary时，就等待租约到期后再重新指定即可。

添加新文件（没有任何chunk与之相关）时，master会新建各种信息、随机指定P和S并进行特殊的创建工作。
## Read

应用想读取一个文件的字节偏移a到b范围内的数据，则把文件名和始终偏移量传给master，由master来找文件名、寻找chunk。

客户端能算得出来自己要读的文件在第几个chunk、要几个chunk，但它不知道chunkserver是哪个。

## Write

（对某个chunk）<u>如果没有primary</u>，master寻找到最新版本，并且决定哪个是primary、哪个是secondary。之后自增版本号，并通知这些P、S。此时会定下租期，告诉P这个primary身份的期限。但版本号的持久化时机老师并没有确切回答。

>只有在无primary时才改变版本号，同时决定P和S。

会有多个client发送请求，而P可以决定执行顺序，不需要S再去调度。

master告诉client哪个是primary。primary接收client的信息，并选择一个偏移量（要保证能存的下）；同时通知其他secondary该偏移量。这样，所有replica就存在了同一个地方。

S会告诉P成功or失败。有任何一个失败就会通知client操作失败，要求client重新走流程（当做上个请求完全没发生）。因此，即使失败，也会有某些cs执行了该写操作，导致能不能读到的随机的。但是，client知道失败后，自然会重新发起请求直到写入成功。只要操作进行到底，就能保证全部写入成功。但每次请求的偏移量都不同，所以之前的偏移量的地方有没有该记录也是随机的。换句话说，保证最终必然有一块一方存储了这个写入数据。

![400](assets/uTools_1680065941129.png)

似乎一般都是加写数据，写在chunk中已存储的最后位置之后。

如果一个文件的顺序很重要，比如电影文件，那就应该连续一次性写完，而不是分多次写。

# Primary-Backup Replication

只能解决fail-stop faults，而不能解决bug。

解决方法：
- State Transfer 复制状态（类似快照）（不关心是否并行）
- Replicated State Machine 传递外部事件（类似日志）（不能用于新建replica）

P/B sync: Backup往往比Primary慢，当P宕机时，B不一定能赶得上进度。

Non-determined Event: 例如生成随机数和获取当前时间，不能交给主从机两个机子做。从机必须拿主机的计算结果。

B只有在知道自己什么时候能停下时，才会开始执行。使用buffer缓存这些指令。

主机要回复client时，等从机接收client的指令并发来ACK后主机才真正回复。

假如出现一种情况，主从先后发了同一个回复给client，也不会出问题。因为回复的时刻主从状态完全相同，则网络包的各种东西都完全相同，在协议里面被视为重传而丢弃。各种方案几乎都可能产生这样的重复输出。

Test and Set Server: 具有一把锁决定哪个机器能上线（成为P）

# Raft

[分布式算法：Raft论文翻译 - 知乎](https://zhuanlan.zhihu.com/p/343560811)

任何时候，服务器是领导者、追随者和候选人三个中的一种状态。在正常操作中，只有一个leader，而其他所有服务器都是follower。

- follower是被动的：他们自己不发出请求，而只是响应leader和candidate的请求。
- leader处理所有客户请求（如果客户联系follower，则follower将其重定向到leader）。 
- candidate，用于选举新的领导者。

![](assets/uTools_1680235452963.png)

每个任期(term)开始一次选举(election)，其中一个或者更多的candidate试图成为leader。 如果candidate在选举中获胜，则在该任期剩余时间中将担任leader。 在某些情况下，选举将导致投票分裂。 在这种情况下，任期将以没有leader的情况结束；新的任期(新的选举)会很短。Raft保证一个任期内最多只有一个leader。

所有人都选自己以及比自己新的，则每一个被选为leader的服务器所包含的日志肯定被复制到了超半数的服务器上，也就都是已提交的日志。（但某一日志即使被复制超半数也不一定是已提交，这在后面会提到）

![](assets/uTools_1680235477123.png)

Raft使用心跳机制来触发leader的选举过程。当服务器开始工作，它们成为follower。只要服务器收到来自leader或者candidate有效RPC，他就会保持follower状态。leader向所有follower发送周期性的心跳信号（不携带日志条目的AppendEntries RPC），以维护其领导地位。如果follower在称为选举超时(election timeout)的时间段内没有任何通信，则它假定没有可行的leader，并开始竞选新leader。

- split votes: 分裂投票

- RequestVote RPC 由Candidate在选举期间启动
- AppendEntries RPC 由Leader启动用来复制日志条目并提供一种形式的心跳

当等待投票的时候，candidate可能收到另一个服务器声明已经成为leader的AppendEntries RPC。如果这个leader的任期不低于这个candidate的任期，这个candidate就承认leader的正当性然后成为follower。如果这个RPC中的任期比candidate的任期小，这个candidate就就拒绝这个RPC，然后继续保持candidate状态。

## 日志

日志由条目组成，这些条目按顺序编号。 每个条目包含创建它的任期（每个框中的数字）和状态机指令。 如果可以安全地将该条目应用于状态机，则认为该条目被提交。每个日志条目还有一个数字索引表示它在日志中的位置。

![](assets/uTools_1680242280464.png)

条目的提交会把以前的所有条目给提交，因为几条记录的是最高索引。该索引会包含在AppendEntries RPC中使得follower都知道哪些条目被提交了。**若得知该条目被提交，则可将其正式应用与本地状态机。**

如果两个在不同日志的条目有相同的索引和任期，那么它们储存同样的指令，且这两个日志中该索引之前的条目都是相同的

发送AppendEntries RPC时，leader在其中包括其日志中下一条新条目之前的索引和任期(当前已提交的最大索引和任期)。 如果follower在其日志中找不到具有相同索引和任期的条目，它拒绝该条目。 一致性检查是一个归纳步骤：日志的初始空白状态满足LogMatching属性，并且每次扩展日志时，一致性检查都会维持Log Matching属性。结果，只要AppendEntries成功返回，leader就会知道follower的日志和自己的新条目之前的日志完全相同。

>在Raft中，leader通过强制follower复制leader的日志来解决不一致问题。这意味着follower中冲突的条目会被leader的日志覆盖。leader会找到两个日志中相同的最新日志条目，然后删除follower日志中该条目之后的所有内容，然后将leader中该条目之后的所有条目发给follower。 所有这些操作用来响应AppendEntriesRPC执行的一致性检查。 
>leader为每个follower维护一个nextIndex，这是leader将发送给该follower的下一个日志条目的索引。当leader刚选举出来时，它将所有nextIndex值初始化为其日志中的最大索引之后的索引（图7中的11）。 如果follower的日志与leader的日志不一致，则下一个AppendEntries RPC中的一致性检查将失败。失败之后，leader递减nextIndex并重试AppendEntries RPC。 最终nextIndex将到达leader和follower日志匹配的点。至此，AppendEntries将成功执行，这将删除follower日志中的所有冲突条目，并从leader的日志中添加附加条目（如果有）。 一旦AppendEntries成功，follower的日志将与leader的日志保持一致，并且在本任期的其余部分中将保持这种状态。

## 安全性

这个算法让**上任leader所有提交的条目在新leader中都存在**（所有已提交的条目都会被leader包含，也就是说**没有被提交后又被废弃的条目**）。而被提交的条目才会被服务器正式应用。

日志条目只有一个流向，从leader到follower，而且<u>leader从不修改它已存在的日志</u>。

>RequestVote RPC 包含有关candidate日志的信息。如果follower自己的日志比candidate的日志新，则拒绝投票。

>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新(up-to-date)。 如果日志中的最后一个条目具有不同的任期，则带有**较新任期**的日志将是最新的。 如果日志以相同的任期结尾，则**更大索引**的日志是最新的。

一个条目即使被复制到了大多数服务器，也不一定是已提交的。若还在复制但未达到提交就宕机、重启后又有新的输入的话，**旧的输入复制再多也不算提交**，只有最新的输入被复制到过半服务器了才算提交（因为这样的话就不会投票给其他leader导致新旧输入都被覆盖）。

![](assets/uTools_1680247505548.png)

如果服务器S1添加（收到AppendEntries）了S2的条目，而S3没有添加S2的条目，此时S1不可能投票（收到RequestVote）给S3。




















